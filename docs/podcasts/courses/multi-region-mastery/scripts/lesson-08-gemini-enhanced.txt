Autonoe: You've built multi-region infrastructure. Aurora replicating. DynamoDB active-active. <say-as interpret-as="characters">EKS</say-as> clusters running. Network layer connecting them. Observability showing what's happening. [pause short] But here's the critical question: When US-EAST-1 fails, how do users actually reach US-WEST-2? [pause]

Autonoe: Your Aurora secondary gets promoted. Your <say-as interpret-as="characters">EKS</say-as> pods are running in the secondary region. Everything is ready. [pause long] But users are still hitting US-EAST-1 endpoints. Their requests are timing out. Revenue is dropping. The missing piece is <say-as interpret-as="characters">DNS</say-as>. [pause short] Route53 health checks, failover policies, TTL configuration. This is your control plane for multi-region traffic. Get it right and failover is transparent. Get it wrong [pause long] and your hot-warm becomes no-warm during actual failures.

Autonoe: Today you're learning how <say-as interpret-as="characters">DNS</say-as> and traffic management actually work in multi-region architectures. Not the marketing promises. The production reality with all the gotchas. By the end of this lesson, you'll know how to configure Route53 health checks that detect failures in under sixty seconds. [pause short] How failover routing policies work when health checks fail. [pause short] When to use weighted routing versus latency-based routing versus geolocation. [pause short] Global Accelerator for subsecond failover with anycast IPs. [pause short] And the <say-as interpret-as="characters">DNS</say-as> pitfalls that caused production outages - TTL caching nightmares, health check false positives, split-brain scenarios.

Autonoe: Let's start with Route53 fundamentals, because most engineers misunderstand how <say-as interpret-as="characters">DNS</say-as> failover actually works. You create an A record pointing to your US-EAST-1 load balancer. Users resolve your domain, get the US-EAST-1 IP, connect. Simple. But what happens when US-EAST-1 fails? [pause]

Autonoe: [pause long]

Autonoe: With basic <say-as interpret-as="characters">DNS</say-as>, nothing. The A record still points to US-EAST-1. Users keep getting that IP. They try to connect, it times out. <say-as interpret-as="characters">DNS</say-as> doesn't know the endpoint is down. It just returns the configured IP.

Autonoe: Route53 health checks solve this. You configure a health check that monitors your US-EAST-1 endpoint. <say-as interpret-as="characters">HTTP</say-as> request every thirty seconds. [pause short] If three consecutive checks fail, Route53 marks it unhealthy. Ninety seconds to detect failure. [pause short] When the primary is marked unhealthy, Route53 starts returning the secondary IP. Users resolve your domain, get US-WEST-2 IP instead. They connect to the secondary region. Failover complete.

Autonoe: But there's latency. Ninety seconds to detect failure. [pause short] Plus <say-as interpret-as="characters">DNS</say-as> TTL. If your TTL is sixty seconds, existing <say-as interpret-as="characters">DNS</say-as> caches don't update for another sixty seconds. [pause short] Total failover time: two and a half minutes from detection. This matches the hot-warm five-minute RTO from Episode 2. One minute Aurora promotion, ninety seconds health check detection, sixty seconds <say-as interpret-as="characters">DNS</say-as> propagation, remaining time for connection establishment and cache warming.

Autonoe: Health check configuration matters. Route53 offers <say-as interpret-as="characters">HTTP</say-as>, <say-as interpret-as="characters">HTTPS</say-as>, TCP health checks. For web applications, use <say-as interpret-as="characters">HTTPS</say-as> checks against a dedicated health endpoint. Don't check your homepage. Check a lightweight endpoint that verifies critical dependencies. Example health endpoint: /health. Returns 200 OK if Aurora is reachable, if the application can connect, if critical services respond. Returns 500 error if anything is broken. This gives you actual application health, not just "is the server responding."

Autonoe: Health check frequency: Every ten seconds costs more than every thirty seconds. For most applications, thirty-second checks are sufficient. [pause short] Faster checks reduce detection time but increase costs and can cause false positives from transient network issues.

Autonoe: Failure threshold: How many consecutive failures before marking unhealthy. Three failures at thirty-second intervals equals ninety seconds. [pause short] Two failures equals sixty seconds but increases false positive risk. Balance detection speed against stability.

Autonoe: Now routing policies, because this is where teams make mistakes. Route53 offers six routing policies. Failover, weighted, latency-based, geolocation, geoproximity, multivalue. Each solves different problems.

Autonoe: [pause]

Autonoe: Failover routing: Primary and secondary records. Health check on primary. When primary fails, return secondary. This is the hot-warm pattern. Use when you have one active region and one standby region. Configuration: Create two A records with the same name. One marked primary with US-EAST-1 IP and health check. One marked secondary with US-WEST-2 IP, no health check. When primary health check fails, Route53 returns secondary.

Autonoe: Weighted routing: Split traffic across multiple regions by percentage. Seventy percent to US-EAST-1, thirty percent to US-WEST-2. Use for gradual migration or load distribution in hot-hot patterns.

Autonoe: Latency-based routing: Route users to lowest-latency region. Route53 measures latency from different <say-as interpret-as="characters">AWS</say-as> regions to users globally. User in California gets routed to US-WEST-2. User in Virginia gets US-EAST-1. Use when you want performance optimization but all regions are active.

Autonoe: Geolocation routing: Route based on user's physical location. All European users to EU-WEST-1 regardless of latency. All US users to US-EAST-1. Use for data residency requirements or regional content.

Autonoe: Common mistake: Using latency-based routing for hot-warm. [pause long] Teams think "latency-based will send traffic to the healthy region." Wrong. Latency-based routes to lowest-latency region. If US-EAST-1 fails but is still responding to <say-as interpret-as="characters">DNS</say-as> queries, users still get routed there based on latency. They then fail to connect. Use failover routing for hot-warm, not latency-based.

Autonoe: TTL configuration is critical. Time To Live tells <say-as interpret-as="characters">DNS</say-as> resolvers how long to cache your answer. Sixty-second TTL means users' <say-as interpret-as="characters">DNS</say-as> caches refresh every sixty seconds. [pause short] Five-minute TTL means caches don't refresh for five minutes. Lower TTL means faster failover propagation but higher query volume. Sixty-second TTL with a million users means frequent re-queries. Higher cost, more load on Route53. Three-hundred-second TTL means slower failover but lower cost. For hot-warm architectures, sixty to one-hundred-twenty-second TTL balances failover speed and cost. For hot-hot with health checks on all regions, you can use longer TTL since all regions are always healthy.

Autonoe: Real production example. E-commerce site, hot-warm with Route53 failover routing. Primary US-EAST-1, secondary US-WEST-2. Health check every thirty seconds against /health endpoint. Sixty-second TTL. [pause long] Black Friday morning. Database issue in US-EAST-1. Application starts returning 500 errors. Route53 health check fails once, twice, three times. Ninety seconds elapsed. [pause short] Route53 marks US-EAST-1 unhealthy. New <say-as interpret-as="characters">DNS</say-as> queries return US-WEST-2 IP. Existing caches expire over the next sixty seconds. Total failover time: two and a half minutes. [pause short] Revenue loss during those minutes: eighty thousand dollars. [pause short] But the alternative - extended outage while manually updating <say-as interpret-as="characters">DNS</say-as> - would have cost millions.

Autonoe: Now Global Accelerator, because <say-as interpret-as="characters">DNS</say-as>-based failover has limitations that Global Accelerator solves. <say-as interpret-as="characters">DNS</say-as> failover depends on TTL. Users cache <say-as interpret-as="characters">DNS</say-as> responses. Even with sixty-second TTL, some resolvers ignore it and cache longer. Mobile networks, corporate <say-as interpret-as="characters">DNS</say-as> servers, they cache aggressively. This extends failover time unpredictably.

Autonoe: Global Accelerator uses anycast IPs. You get two static IPs that announce from all <say-as interpret-as="characters">AWS</say-as> edge locations globally. Users connect to the nearest edge location. Edge location maintains persistent connections to your healthy endpoints. When US-EAST-1 fails, Global Accelerator detects it via health checks. Edge locations stop routing to US-EAST-1. They route to US-WEST-2 instead. [pause long] No <say-as interpret-as="characters">DNS</say-as> involved. No TTL wait. Subsecond failover.

Autonoe: How it works: You create a Global Accelerator accelerator. You add two endpoint groups - US-EAST-1 and US-WEST-2. You configure health checks. Global Accelerator assigns you two anycast IPs. Users connect to those IPs. Traffic routes through <say-as interpret-as="characters">AWS</say-as> edge network to your healthy endpoints. When US-EAST-1 health check fails, the edge locations immediately stop sending traffic there. They send everything to US-WEST-2. Existing TCP connections might drop, but new connections go to the healthy region. Failover time: under ten seconds. [pause short]

Autonoe: Cost: Global Accelerator charges per accelerator and per gigabyte of data transfer. About twenty-five dollars per month per accelerator, plus eight cents per gigabyte. [pause short] For high-traffic sites where subsecond failover matters, this is worth it. For lower-traffic sites where two-minute <say-as interpret-as="characters">DNS</say-as> failover is acceptable, stick with Route53.

Autonoe: [pause]

Autonoe: When to use Global Accelerator over Route53:
Autonoe: You need subsecond failover. You have users with aggressive <say-as interpret-as="characters">DNS</say-as> caching that breaks TTL. You want consistent IPs that don't change during failover. You're serving TCP/UDP protocols where <say-as interpret-as="characters">DNS</say-as>-based failover is clunky.

Autonoe: When to stick with Route53:
Autonoe: Cost-sensitive. Two-minute failover is acceptable for your RTO. <say-as interpret-as="characters">HTTP</say-as>/<say-as interpret-as="characters">HTTPS</say-as> traffic where <say-as interpret-as="characters">DNS</say-as> failover works well.

Autonoe: Common <say-as interpret-as="characters">DNS</say-as> mistakes that break multi-region.
Autonoe: [pause]

Autonoe: Mistake one: [pause long] No health checks on failover records. Teams create primary and secondary records but forget to add health checks. When primary fails, Route53 keeps returning it because it doesn't know it's down. Fix: Always attach health checks to failover primary records.

Autonoe: Mistake two: [pause long] Health checks that don't test real application health. Teams check TCP port 80 is open. Server is up, but application is broken. Route53 thinks it's healthy. Users get errors. Fix: Use health endpoints that verify critical dependencies - database connectivity, external <say-as interpret-as="characters">API</say-as> availability, authentication service.

Autonoe: Mistake three: [pause long] Single health check location. Route53 health checks run from multiple locations by default, but you can configure them to check from specific regions. If you only check from one location, network issues in that location cause false positives. Fix: Use default multi-location health checks unless you have specific regional requirements.

Autonoe: Mistake four: [pause long] Ignoring <say-as interpret-as="characters">DNS</say-as> resolver caching. Teams set sixty-second TTL but expect instant failover. They don't account for resolvers that ignore TTL or have minimum cache times. Mobile networks often cache for five to fifteen minutes regardless of TTL. Fix: Set expectations correctly. <say-as interpret-as="characters">DNS</say-as> failover is minutes, not seconds. Use Global Accelerator if you need faster.

Autonoe: Mistake five: [pause long] Split-brain during failover. Primary region is partially failed. Some services work, others don't. Health check happens to hit working services, returns healthy. But most user requests hit broken services. Chaos. Fix: Health checks must verify end-to-end flow, not just individual components.

Autonoe: Before we wrap up, pause and answer these questions. [pause short]
Autonoe: Question one: Your hot-warm architecture needs to failover when US-EAST-1 fails. Should you use failover routing, weighted routing, or latency-based routing? [pause]
Autonoe: Question two: You set TTL to sixty seconds. Route53 health check takes ninety seconds to detect failure. How long until all users are fully failed over to secondary region? [pause]
Autonoe: Question three: When would you use Global Accelerator instead of Route53 for failover? [pause]

Autonoe: Take a moment. [pause short] Answers.

Autonoe: Question one: Failover routing. This is the exact use case for failover routing - primary region with health check, secondary region without. Weighted routing doesn't consider health, it just splits traffic by percentage. Latency-based routing routes to lowest latency, not to healthy region. For hot-warm, always use failover routing.

Autonoe: Question two: About two and a half to three minutes for most users. Ninety seconds for health check detection. [pause short] Sixty seconds for <say-as interpret-as="characters">DNS</say-as> cache expiration. [pause short] Plus connection establishment time. Some users with aggressive caching might take longer. This aligns with the five-minute RTO for hot-warm from Episode 2.

Autonoe: Question three: When you need subsecond failover and can't tolerate <say-as interpret-as="characters">DNS</say-as> TTL delays. When you have users behind resolvers with aggressive caching. When you want static IPs that don't change. When cost isn't the primary concern - Global Accelerator costs more than Route53 but delivers faster failover.

Autonoe: Let's recap what we covered.
Autonoe: [pause]

Autonoe: First: Route53 health checks detect failures by polling endpoints every ten to thirty seconds. Three consecutive failures marks unhealthy, typically ninety seconds detection time. [pause short] Health checks should verify real application health, not just server uptime.

Autonoe: Second: Failover routing policy is correct for hot-warm architectures. Primary record with health check, secondary without. When primary fails, Route53 returns secondary IP. Don't use latency-based routing for this - it routes by latency, not health.

Autonoe: Third: TTL controls <say-as interpret-as="characters">DNS</say-as> cache duration. Sixty-second TTL balances failover speed and query cost. Lower TTL means faster propagation but more queries. Total failover time is health check detection plus TTL expiration.

Autonoe: Fourth: Global Accelerator provides subsecond failover using anycast IPs and edge network routing. Costs more than Route53 but eliminates <say-as interpret-as="characters">DNS</say-as> TTL delays. Use when subsecond failover is required.

Autonoe: Fifth: Common mistakes - no health checks, health checks that don't test real application health, ignoring <say-as interpret-as="characters">DNS</say-as> caching behavior, single-location checks, split-brain scenarios during partial failures.

Autonoe: Remember Episode 2's hot-warm pattern with five-minute RTO? The breakdown was one minute Aurora promotion, ninety seconds health check detection, sixty seconds <say-as interpret-as="characters">DNS</say-as> propagation, remaining time for connection warming. [pause short] Now you understand exactly how those ninety seconds and sixty seconds work in Route53.

Autonoe: We'll use this <say-as interpret-as="characters">DNS</say-as> configuration in Episode 12 when we walk through actual disaster recovery procedures. You'll see how to test failover without affecting production, how to monitor health check status during incidents, and what to do when <say-as interpret-as="characters">DNS</say-as> failover doesn't work as expected.

Autonoe: Next time: Cost Management - optimizing the seven-point-five times multiplier. Episodes 1 through 8 gave you

Autonoe: the building blocks. Aurora, DynamoDB, <say-as interpret-as="characters">EKS</say-as>, networking, observability, <say-as interpret-as="characters">DNS</say-as>. You know how to build multi-region. But what does it actually cost? [pause]

Autonoe: You'll learn why multi-region costs seven-point-five times more than single-region, [pause short] not the two times vendors claim.

Autonoe: [pause]

Autonoe: The hidden cost multipliers - data transfer, replication, redundant compute, higher operational overhead. Cost optimization strategies that cut the multiplier from seven-point-five to four without sacrificing reliability. When to use reserved capacity versus on-demand.

Autonoe: And the make-or-break calculation: Is multi-region worth it for your revenue and risk tolerance?

Autonoe: [pause long]

Autonoe: Because you can build a technically perfect multi-region architecture that's financially unsustainable. Engineering succeeded but the business failed. Let's make sure you avoid that.

Autonoe: See you in Episode 9.