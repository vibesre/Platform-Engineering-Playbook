Welcome to Lesson 2 of <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> Production Mastery.

Remember that production readiness checklist from last time? Item number one?

Resource limits and requests.

Set on every single container. Every one.

But here's what I've noticed talking to engineers over the years - and honestly, this was me too when I started - most people can't explain WHY you need both. They know you're supposed to set them. They've copied <phoneme alphabet="ipa" ph="ˈjæməl">YAML</phoneme> from Stack Overflow. But they don't understand the actual mechanism.

That changes today.

Let me ask you something. That Black Friday incident from Episode 1 - ninety-four thousand dollars lost in forty-seven minutes. Do you remember what caused it?

OOMKilled pods. Exit code 137. Resource limits missing.

Here's the brutal truth about <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> production. Exit code 137 - OOMKilled - is the single most common failure. Sixty-seven percent of teams running <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> have hit this. That's not some edge case. That's the norm.

If you don't master resource management, you WILL get paged at three AM. Not might. Will.

By the end of these next fifteen minutes, you'll understand requests versus limits at a level where you can debug OOMKilled errors in production. You'll be able to architect resource allocation for entire clusters. And you'll make intelligent trade-off decisions instead of guessing.

Let's start with the thing that trips up most engineers.

Requests and limits. People think they do the same thing.

They don't.

Requests are for the scheduler. Limits are for the runtime.

Two completely different systems doing two completely different jobs.

Here's an analogy that finally made this click for me back when I was debugging a cascade failure at two AM.

Think of resource requests like booking a hotel room. You request a room with one king bed. The hotel guarantees that room exists. It's yours. That's your request - it's a promise from the system that these resources will be available when you need them.

Now, just because you requested a king bed doesn't mean you'll USE the whole room, right? Maybe you only sleep on half the bed. Maybe that mini-bar stays closed. But the hotel still reserves that entire room for you. That's scheduling. That's capacity planning.

Resource limits? That's the fire marshal walking through and saying "This room has a maximum occupancy of four people. Period." Try to cram in a fifth person? Fire code violation. System says no. Doesn't matter what you want. The rules enforce.

In <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme>, requests ensure your pod gets scheduled on a node with sufficient resources available. Limits prevent your pod from consuming unlimited resources and taking down other workloads.

Let me show you what this looks like with real numbers from an actual production system.

You've got an <say-as interpret-as="characters">API</say-as> pod. Here's the configuration:

```yaml
resources:
  requests:
    memory: 512Mi
    cpu: 250m
  limits:
    memory: 1Gi
    cpu: 500m
```

What does this actually mean in practice?

The scheduler - that's the <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> component deciding which node gets your pod - looks at this and thinks: "I need a node with at least 512 megabytes of available memory and 250 millicores of available <say-as interpret-as="characters">CPU</say-as>." That's the scheduling guarantee. The scheduler won't place this pod on a node that doesn't have that much capacity available.

Once your pod is running, the container runtime takes over. That's containerd, CRI-O, whatever you're using. It enforces the limits. Your pod can USE up to 1 gigabyte of memory and 500 millicores of <say-as interpret-as="characters">CPU</say-as>.

What happens if it tries to exceed 1 gigabyte?

OOMKilled. Exit code 137. Instantly. No warnings. No grace period. Done.

Here's what this looks like in a production traffic pattern.

Normal load? Your pod uses 400 megabytes. Perfect. Well within both the request and the limit.

Traffic spike hits. Memory climbs to 800 megabytes. Still fine. You're between your request of 512 and your limit of 1024. This is exactly what burstability is for.

But then something happens. Maybe it's Black Friday and traffic just exploded. Maybe there's a memory leak you haven't caught yet. Memory hits 1.1 gigabytes.

<phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> kills your pod. Immediately.

Now you're probably thinking - okay, just set limits really high. Problem solved, right?

Wrong. Because then you overcommit nodes, waste massive amounts of money, and create what we call the noisy neighbor problem where one pod's spike takes down three others.

We'll get to that. But first, let me show you how this actually works under the hood.

When you deploy a pod with resource requests, the <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> scheduler kicks in.

It looks at all your nodes. For each node, it does math: total capacity minus already-requested resources equals available capacity.

Then it finds a node where available capacity is greater than or equal to your request. That's where your pod goes.

Let me give you a concrete example from a cluster I was debugging last month.

Node has 8 gigabytes of total memory. Existing pods on that node have requested 5 gigabytes. Your new pod requests 2 gigabytes.

Scheduler does the math: 8 minus 5 equals 3 gigabytes available. Your 2 gigabyte request fits. Pod gets scheduled there.

Here's the gotcha that confuses people.

That node might only have 1 gigabyte of memory ACTUALLY FREE right now. Because requests are guarantees, not actual usage. Those existing pods might only be using 3 gigabytes even though they requested 5.

This is called overcommitment. And it's intentional. <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> is designed for this. Otherwise you'd waste huge amounts of capacity.

Now let's talk about how limits get enforced. Because this is where OOMKills actually happen.

Limits are enforced by the container runtime using <phoneme alphabet="ipa" ph="ˈlɪnəks">Linux</phoneme> control groups - cgroups. Every container gets its own cgroup. That cgroup has memory limits configured directly in the kernel.

When your process tries to allocate memory beyond that limit, the <phoneme alphabet="ipa" ph="ˈlɪnəks">Linux</phoneme> OOMKiller steps in. Not <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme>. The kernel.

Let me walk you through exactly what happens when a pod gets OOMKilled. Because understanding this will save you hours of debugging.

Your pod is running, using 800 megabytes of memory. Everything's normal. Then your application gets a burst of traffic. It tries to allocate another 300 megabytes to handle the requests.

Total attempted: 800 plus 300 equals 1.1 gigabytes.

Your configured limit: 1 gigabyte.

The kernel checks. Attempted allocation of 1.1 gigabytes exceeds limit of 1.0 gigabytes.

Kernel invokes the OOMKiller. It sends SIGKILL - that's signal 9. The kill signal. No cleanup. No graceful shutdown. Just terminated.

Exit code becomes 128 plus 9 equals 137.

Pod status: OOMKilled.

<phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> sees the failure. Your restart policy is set to Always. So it starts a new pod.

If the problem persists? That new pod tries, uses too much memory, gets killed. Tries again, killed again. Now you're in CrashLoopBackOff. Pod keeps dying, <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> keeps restarting it with increasing backoff delays.

This is production at three AM.

Now here's something that trips up even experienced engineers who think they understand this stuff.

Nodes can be overcommitted on requests. But NOT on limits.

Let me show you a scenario I've seen play out multiple times.

You've got a node with 8 gigabytes of memory. Three pods on that node. Each pod requests 2 gigabytes and limits at 4 gigabytes.

Total requests: 6 gigabytes. Fits on the node. Scheduler is happy.

Total limits: 12 gigabytes. Does NOT fit on the node.

Is this a problem?

Usually not. Most pods don't hit their limits. That's why they're limits - they're safety valves, not operating targets.

But what if all three pods spike to their limits at the same time? Traffic surge. Memory leak. Whatever.

The node runs out of physical memory. Kernel starts killing pods. Random pods. Production outage. Pages flying. Engineers scrambling.

This is why right-sizing matters. This is why monitoring matters. This is why we have Quality of Service classes - which we'll get to in a minute.

First, let me show you the systematic five-step debugging workflow that will save your ass when pods start OOMKilling.

Real incident. Twenty twenty-four. Black Friday. E-commerce company. Traffic spiking hard.

Three AM, engineer's pager goes off. <say-as interpret-as="characters">API</say-as> pods OOMKilling in a cascade. One pod dies, others get more traffic, their memory spikes, they die too. Cascading failure.

The on-call engineer <say-as interpret-as="characters">SSH</say-as>'s into a node. Panicking. Runs random <phoneme alphabet="ipa" ph="ˈkubkənˌtroʊl">kubectl</phoneme> commands. Makes it worse.

Here's the workflow I wish they'd followed.

Step one: Identify the symptom.

Run: `<phoneme alphabet="ipa" ph="ˈkubkənˌtroʊl">kubectl</phoneme> get pods -n production`

You see: `api-deployment-7f6b8c9d4-x7k2m   0/1   OOMKilled   3   5m`

Exit code 137. Memory issue confirmed. Not a crash. Not a config problem. Memory.

Step two: Check the events.

Run: `<phoneme alphabet="ipa" ph="ˈkubkənˌtroʊl">kubectl</phoneme> describe pod api-deployment-7f6b8c9d4-x7k2m -n production`

Scroll to events. You see: `OOMKilled: Container exceeded memory limit (1073741824 bytes)`

Translation: hit 1 gigabyte limit. But we don't know yet if the limit is too low or if there's a memory leak in the app.

Step three: Check actual usage patterns.

Run: `<phoneme alphabet="ipa" ph="ˈkubkənˌtroʊl">kubectl</phoneme> top pod` for current usage if the pod is still running. But you need historical data.

Pull up <phoneme alphabet="ipa" ph="prəˈmiθiəs">Prometheus</phoneme>. Or Datadog. Or whatever monitoring you have. Query: `container_memory_usage_bytes` for that pod name.

The graph tells the story. Normal usage: 600 megabytes. Under load: spikes to 1.2 gigabytes.

Now we're getting somewhere.

Step four: Analyze the configuration.

Run: `<phoneme alphabet="ipa" ph="ˈkubkənˌtroʊl">kubectl</phoneme> get pod api-deployment-7f6b8c9d4-x7k2m -o yaml | grep -A 10 resources`

Shows:
```yaml
requests:
  memory: 256Mi
limits:
  memory: 1Gi
```

Problem identified. Limit is 1 gigabyte. Normal usage is 600 megabytes. That leaves only 400 megabytes of headroom. Traffic spike exceeds that headroom. Pod gets killed.

Step five: Determine root cause and fix.

Two possibilities here. Either the limit is too low for legitimate traffic spikes. Or there's a memory leak in your application code.

In this case? Legitimate traffic spike. Black Friday. Happens every year.

The fix:
```yaml
requests:
  memory: 512Mi  # Increased for better scheduling
limits:
  memory: 2Gi    # Increased for headroom under load
```

Deploy the change. Monitor. Problem solved.

Before we move on, pause and think through this. If you were debugging an OOMKilled pod right now, what would your workflow be?

That's right: get pods to identify, describe pod for events, check monitoring for historical usage, get the <phoneme alphabet="ipa" ph="ˈjæməl">YAML</phoneme> to see configuration, determine if limit is too low or if there's a leak.

Internalize that workflow. Write it down if you need to. You'll use it.

Now let's talk about Quality of Service classes. Because when a node runs out of memory, <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> has to decide which pods to kill. QoS classes determine that priority.

Three classes exist: Guaranteed, Burstable, and BestEffort.

Guaranteed happens when your requests equal your limits for BOTH memory and <say-as interpret-as="characters">CPU</say-as>.

```yaml
requests:
  memory: 1Gi
  cpu: 500m
limits:
  memory: 1Gi
  cpu: 500m
```

These pods are last to be killed during node memory pressure. Use this for your critical workloads. Databases. Payment processing. Anything that absolutely cannot tolerate getting killed.

Burstable is when requests are less than limits.

```yaml
requests:
  memory: 512Mi
limits:
  memory: 2Gi
```

These get killed before Guaranteed but after BestEffort. This is the most common class. Most application workloads should be Burstable. They can burst when they need to, but they have a safety limit.

BestEffort is when you don't set any requests or limits at all.

These are first to be killed during node memory pressure. <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> makes zero promises. Use this for batch jobs, non-critical background tasks, things that can tolerate interruption.

Pro tip: almost never use BestEffort in production. You're asking for trouble.

When a node runs out of memory, <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> kills pods in this order:

BestEffort first. No guarantees, they're gone.

Then Burstable pods that are exceeding their requests. They're using more than they promised.

Then Burstable pods within their requests.

Finally, Guaranteed pods. Last resort.

This is why setting requests and limits correctly matters. It's not just resource allocation. It's survival priority during outages.

Now let's talk about actually right-sizing these values. Because this is where theory meets production reality.

Set them too low? OOMKills. Set them too high? Wasted resources, higher costs, poor node utilization.

Here's my three-step process.

Step one: Measure current usage in production for at least a week. Monitor actual memory and <say-as interpret-as="characters">CPU</say-as> consumption. Look at percentiles.

P50 - that's median - shows your typical load. P95 shows high load, not peak. P99 shows peak load, those rare spikes.

Example from a real <say-as interpret-as="characters">API</say-as> service: P50 at 400 megabytes. P95 at 800 megabytes. P99 at 1.2 gigabytes.

Step two: Set requests at P50 to P75. Set limits at P95 to P99 plus twenty to thirty percent headroom.

Requests should be at typical load. This ensures good scheduling without over-requesting resources from the scheduler.

Limits should be at high load with headroom. P95 to P99 plus that buffer.

From our example: Request 512 megabytes - that's around P75. Limit 1.5 gigabytes - that's P99 plus headroom.

This allows normal bursting while preventing runaway memory consumption.

Step three: Load test and validate before production.

```bash
# Simulate three times expected traffic
hey -n 100000 -c 100 https://api.example.com/endpoint
```

Monitor memory during the test. Does it stay within limits? Good. Does it OOMKill? Increase limits, retest.

Critical point: test with realistic traffic patterns. Bursts, not constant load. Production doesn't send constant load.

Let me give you a real cost example of what happens when you don't right-size.

Startup I consulted for. Twenty pods. Each requesting 4 gigabytes. Actual usage? 800 megabytes average.

They're paying for 80 gigabytes of requested capacity. Actually using 16 gigabytes. Wasting twenty-four hundred dollars per month on unused capacity.

Right-sized those pods to 1 gigabyte requests, 2 gigabyte limits. Same performance. Eighteen hundred dollars per month in savings.

At scale, this adds up fast.

Now the three most common mistakes I see with resource management.

Mistake one: setting requests equal to limits for everything. Why it fails: you lose burstability. Everything becomes Guaranteed QoS. You waste resources because most workloads don't need their limits constantly. Fix: use Burstable for most workloads. Requests at typical usage, limits at peak plus headroom.

Mistake two: no limits at all. Why it fails: one memory leak brings down the entire node. Real example: background worker with a bug. Memory climbs - 1 gigabyte, 2 gigabytes, 4 gigabytes, 8 gigabytes. Consumes the entire node. Fourteen other pods on that node start failing. Production outage. Fix: always set limits. Even generous limits are a safety net.

Mistake three: not monitoring actual usage before setting values. Why it fails: you're guessing. Either too low which gives you OOMKills, or too high which wastes money. Fix: deploy with conservative estimates, monitor for a week, adjust based on actual P95 and P99 data.

Before we wrap, let's test your understanding. Pause and answer these without looking back.

One: What's the difference between requests and limits? One sentence each.

Two: A pod has requests of 512 megabytes, limits of 1 gigabyte. It's currently using 750 megabytes. What QoS class?

Three: Node runs out of memory. What order are pods killed?

[pause for answers]

Here they are.

One: Requests are what the scheduler uses to decide which node has capacity. Limits are what the runtime enforces to prevent runaway resource consumption.

Two: Burstable. Requests are less than limits.

Three: BestEffort first, then Burstable exceeding requests, then Burstable within requests, finally Guaranteed.

Let's recap.

Requests versus limits. Requests for scheduling. Limits for enforcement. Two different systems, two different jobs.

The hotel analogy. Requests reserve the room. Limits enforce the fire code.

OOMKilled debugging workflow. Five steps: get pods, describe pod, check events, check monitoring, analyze config, determine root cause and fix.

Quality of Service classes. Guaranteed when requests equal limits. Burstable when requests less than limits. BestEffort when nothing set. Determines eviction priority during memory pressure.

Right-sizing formula. Requests at P50 to P75. Limits at P95 to P99 plus twenty to thirty percent headroom. Validate with load testing.

The costs. Ninety-four thousand dollars from missing limits on Black Friday. Twenty-four hundred dollars per month wasted from over-provisioning.

Remember that production readiness checklist from Episode 1? Resource limits and requests - item number one. Now you understand why. It's not just a checkbox. It's the difference between stable production and three AM pages.

We'll come back to these concepts in Episode 8 when we tackle cost optimization. Right-sizing isn't just about stability. It's about money. And in Episode 4, we'll use this exact debugging workflow to troubleshoot CrashLoopBackOff.

Next: Security Foundations. RBAC and Secrets.

You'll learn why RBAC misconfigurations are still the number one <phoneme alphabet="ipa" ph="ˌkubɚˈnɛtiz">Kubernetes</phoneme> security issue in twenty twenty-five. How to implement least privilege roles that actually work in production. Secrets management patterns using Sealed Secrets and External Secrets Operator.

Remember that crypto miner attack from Episode 1? Forty-three thousand dollar <say-as interpret-as="characters">AWS</say-as> bill from over-privileged service accounts?

That's what we're preventing in Episode 3.

See you then.
