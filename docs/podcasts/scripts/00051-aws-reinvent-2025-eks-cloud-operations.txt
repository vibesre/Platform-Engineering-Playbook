Jordan: Welcome to part three of our four-part AWS re:Invent twenty twenty-five series. <break time="0.4s"/> In episode forty-nine, we covered agentic AI and frontier agents. <break time="0.3s"/> Episode fifty dove into Graviton five, Trainium three, and Lambda Durable Functions. <break time="0.4s"/> Today, we're covering what I think is the most consequential announcement for platform engineers: <break time="0.3s"/> how AWS is transforming E K S from a container orchestrator into a full AI infrastructure platform. <break time="0.3s"/> But first, let's check what else is happening in platform engineering.

Alex: And there's actually some urgent news today. <break time="0.4s"/> cert-manager version one point nineteen point two just dropped, and it's patching two C V Es in the Go standard library. <break time="0.3s"/> C V E twenty twenty-five sixty-one seven two seven and C V E twenty twenty-five sixty-one seven two nine. <break time="0.4s"/> If you're running Trivy scans, these will get flagged on your next run.

Jordan: <break time="0.3s"/> This is a patch now situation, not a next sprint situation. <break time="0.4s"/> The good news is it's a straightforward version bump with no breaking changes. <break time="0.3s"/> And if you're still on the one point eighteen branch, version one point eighteen point four backports the same fixes.

Alex: In other news, Canonical just extended Kubernetes long-term support to fifteen years. <break time="0.4s"/> Fifteen years. <break time="0.3s"/> They're doing this through Ubuntu Pro plus a Legacy add-on, and they're clearly targeting enterprises that need mainframe-level support cycles.

Jordan: <break time="0.3s"/> That's fascinating positioning. <break time="0.4s"/> Meanwhile, Open Tofu one point eleven dropped with a feature I've been waiting for: <break time="0.3s"/> ephemeral resources. <break time="0.4s"/> You can now handle secrets without them persisting to state files. <break time="0.3s"/> They also added an enabled meta-argument to replace those awkward count conditionals we've all written.

Alex: Cloudflare published a detailed post about how they manage hundreds of internal accounts with Infrastructure as Code. <break time="0.4s"/> They're implementing shift-left security to prevent misconfigurations that could propagate globally in seconds. <break time="0.3s"/> Worth reading if you're running IaC at scale.

Jordan: <break time="0.3s"/> And one more quick hit: <break time="0.3s"/> there's a Go proposal gaining traction for runtime secret mode. <break time="0.4s"/> It would protect sensitive values from memory dumps and debugging. <break time="0.3s"/> Important for anyone handling credentials in Go services.

Alex: <break time="0.5s"/> Alright, let's dive into re:Invent. <break time="0.4s"/> Jordan, when you say E K S is becoming an AI infrastructure platform, what does that actually mean?

Jordan: <break time="0.4s"/> It means AWS made four massive announcements that fundamentally change what E K S can do. <break time="0.5s"/> First, E K S Ultra Scale, which supports one hundred thousand nodes in a single cluster. <break time="0.4s"/> Second, E K S Capabilities, which brings managed Argo C D, A C K, and K R O. <break time="0.3s"/> Third, the E K S M C P Server for natural language cluster management. <break time="0.4s"/> And fourth, Provisioned Control Plane with guaranteed performance tiers.

Alex: <break time="0.3s"/> Let's start with Ultra Scale because the numbers are staggering. <break time="0.4s"/> One hundred thousand nodes per cluster. <break time="0.3s"/> To put that in context, Google Kubernetes Engine maxes out at fifteen thousand nodes per standard cluster. <break time="0.4s"/> Azure Kubernetes Service caps at five thousand.

Jordan: <break time="0.4s"/> Right. AWS now has a six to twenty X advantage in cluster scale. <break time="0.5s"/> But the real number that matters isn't nodes, it's accelerators. <break time="0.4s"/> One hundred thousand nodes means you can run up to one point six million AWS Trainium accelerators in a single cluster. <break time="0.3s"/> Or eight hundred thousand NVIDIA G P Us.

Alex: <break time="0.3s"/> That's the scale you need for training trillion-parameter models. <break time="0.4s"/> And these AI workloads fundamentally can't be distributed across multiple clusters easily. <break time="0.3s"/> The training job needs all accelerators available within one logical cluster.

Jordan: <break time="0.5s"/> Here's where it gets technically interesting. <break time="0.4s"/> The bottleneck at scale has always been etcd. <break time="0.3s"/> The Kubernetes control plane stores all cluster state in etcd, and etcd uses Raft consensus for replication. <break time="0.4s"/> Raft works great at normal scale, but it becomes the limiting factor when you're trying to coordinate one hundred thousand nodes.

Alex: <break time="0.3s"/> So what did AWS do?

Jordan: <break time="0.4s"/> They completely overhauled etcd. <break time="0.5s"/> They replaced the Raft-based consensus backend with something they call journal. <break time="0.3s"/> This is an internal component AWS has been building for over a decade. <break time="0.4s"/> It provides ultra-fast, ordered data replication with multi-Availability Zone durability.

Alex: <break time="0.3s"/> So they essentially swapped out the replication engine under etcd.

Jordan: <break time="0.4s"/> Exactly. And they went further. <break time="0.5s"/> They moved etcd's entire backend database to in-memory storage using tmpfs. <break time="0.3s"/> This gives order-of-magnitude performance wins. <break time="0.4s"/> Higher read-write throughput, predictable latencies, faster maintenance operations. <break time="0.3s"/> They also doubled the maximum database size to twenty gigabytes.

Alex: <break time="0.3s"/> What about pod scheduling at that scale? <break time="0.4s"/> The scheduler becomes a bottleneck too.

Jordan: <break time="0.4s"/> AWS optimized the scheduler plugins and node filtering parameters specifically for large-scale workloads. <break time="0.5s"/> The result: <break time="0.3s"/> they achieved five hundred pods per second scheduling throughput even at one hundred thousand node scale.

Alex: <break time="0.3s"/> That's impressive. <break time="0.4s"/> Are there real customers running at this scale?

Jordan: <break time="0.5s"/> Yes. <break time="0.3s"/> Anthropic is using E K S Ultra Scale to train Claude. <break time="0.4s"/> They reported that their end-user latency K P Is improved from an average of thirty-five percent to consistently above ninety percent. <break time="0.3s"/> Amazon is also using it for training Nova models.

Alex: <break time="0.5s"/> Let's move to E K S Capabilities because this affects more teams directly. <break time="0.4s"/> What problem is this solving?

Jordan: <break time="0.4s"/> Platform teams have been running Argo C D for GitOps and A C K for managing AWS resources from Kubernetes. <break time="0.3s"/> But maintaining these systems is real work. <break time="0.4s"/> You need to patch them, upgrade them, ensure compatibility, handle scaling. <break time="0.3s"/> E K S Capabilities makes all of that AWS's problem.

Alex: <break time="0.3s"/> So AWS is now managing Argo C D for you?

Jordan: <break time="0.5s"/> Yes. <break time="0.3s"/> And it's not just running in your cluster. <break time="0.4s"/> These capabilities actually run in AWS service-owned accounts that are fully abstracted from you. <break time="0.3s"/> AWS handles infrastructure scaling, patching, updates, and compatibility analysis.

Alex: <break time="0.3s"/> Let's break down the three capabilities. <break time="0.4s"/> First is Argo C D.

Jordan: <break time="0.4s"/> Argo C D is the most popular GitOps tool for Kubernetes. <break time="0.5s"/> The twenty twenty-four C N C F survey showed forty-five percent of Kubernetes users are running Argo C D in production or planning to. <break time="0.4s"/> With E K S Capabilities, you get a fully managed Argo C D instance that can deploy applications across multiple clusters. <break time="0.3s"/> Git becomes your source of truth, and Argo automatically remediates drift.

Alex: <break time="0.3s"/> Second is A C K, AWS Controllers for Kubernetes.

Jordan: <break time="0.4s"/> A C K lets you manage AWS resources using Kubernetes Custom Resource Definitions. <break time="0.5s"/> The managed version provides over two hundred C R Ds for more than fifty AWS services. <break time="0.3s"/> You can create S three buckets, R D S databases, I A M roles, all from YAML. <break time="0.4s"/> No need to install or maintain controllers yourself.

Alex: <break time="0.3s"/> And third is K R O, Kube Resource Orchestrator.

Jordan: <break time="0.5s"/> K R O is about abstraction. <break time="0.3s"/> Platform teams can create reusable resource bundles that hide complexity. <break time="0.4s"/> Developers consume these abstractions without needing to understand the underlying details. <break time="0.3s"/> It's how you build your internal developer platform on Kubernetes.

Alex: <break time="0.4s"/> What does the multi-cluster architecture look like?

Jordan: <break time="0.5s"/> You run all three capabilities in a centrally managed cluster. <break time="0.3s"/> Argo C D on that management cluster deploys applications to workload clusters across different regions or accounts. <break time="0.4s"/> A C K provisions AWS resources for all clusters. <break time="0.3s"/> K R O creates portable platform abstractions that work everywhere. <break time="0.4s"/> It's a clean separation of concerns.

Alex: <break time="0.3s"/> How does pricing work?

Jordan: <break time="0.4s"/> Per-capability, per-hour billing with no upfront commitments. <break time="0.5s"/> You're charged for each capability resource for each hour it's active. <break time="0.3s"/> There are also additional charges for the specific Kubernetes resources managed by the capabilities.

Alex: <break time="0.5s"/> Now let's talk about the E K S M C P Server because this feels like a glimpse of the future. <break time="0.4s"/> What is M C P?

Jordan: <break time="0.4s"/> M C P stands for Model Context Protocol. <break time="0.3s"/> It's an open-source standard that gives AI models secure access to external tools and data sources. <break time="0.4s"/> Think of it as a standardized interface that enriches AI applications with real-time, contextual knowledge.

Alex: <break time="0.3s"/> And AWS built an M C P server for E K S?

Jordan: <break time="0.5s"/> Yes. <break time="0.3s"/> The E K S M C P Server lets you manage Kubernetes clusters using natural language instead of kubectl. <break time="0.4s"/> You can say show me all pods not in running state, and it just works. <break time="0.3s"/> You can say create a new E K S cluster named demo cluster with V P C and Auto Mode, and it does it.

Alex: <break time="0.3s"/> No kubectl, no kubeconfig?

Jordan: <break time="0.4s"/> Exactly. <break time="0.5s"/> You can get logs, check deployments, create clusters, all through conversation. <break time="0.3s"/> The server translates natural language into AWS and Kubernetes API calls behind the scenes.

Alex: <break time="0.3s"/> What about security and enterprise requirements?

Jordan: <break time="0.5s"/> The M C P server is hosted in AWS cloud, so there's no local installation or maintenance. <break time="0.3s"/> You get automatic updates, patching, AWS I A M integration for security, and CloudTrail integration for audit logging. <break time="0.4s"/> Plus, it draws on a knowledge base built from AWS's operational experience managing millions of Kubernetes clusters.

Alex: <break time="0.3s"/> What AI tools does it integrate with?

Jordan: <break time="0.4s"/> It works with Kiro, which is AWS's IDE and CLI. <break time="0.5s"/> Also Cursor, Cline, and you can use it with Amazon Q Developer. <break time="0.3s"/> Or you can build your own agents that interface with E K S clusters through the M C P server.

Alex: <break time="0.5s"/> This changes who can operate Kubernetes clusters.

Jordan: <break time="0.4s"/> That's the key insight. <break time="0.3s"/> AWS was one of the first managed Kubernetes providers to implement M C P. <break time="0.4s"/> They're betting that conversational AI turns multi-step manual tasks into simple requests. <break time="0.3s"/> The barrier to Kubernetes operations just dropped significantly.

Alex: <break time="0.5s"/> Let's cover Provisioned Control Plane before we move to cloud operations. <break time="0.4s"/> What problem does this solve?

Jordan: <break time="0.4s"/> Standard E K S control planes have variable performance. <break time="0.3s"/> Under burst loads, you can get unpredictable behavior. <break time="0.4s"/> Enterprises need guaranteed SLAs, especially for production workloads. <break time="0.5s"/> Provisioned Control Plane lets you pre-allocate control plane capacity with well-defined performance characteristics.

Alex: <break time="0.3s"/> How do the tiers work?

Jordan: <break time="0.4s"/> They use T-shirt sizing: <break time="0.3s"/> X L, two X L, and four X L. <break time="0.4s"/> Each tier defines capacity through three attributes. <break time="0.3s"/> First, API request concurrency, which is how many requests the API server can handle simultaneously. <break time="0.4s"/> Second, pod scheduling rate in pods per second. <break time="0.3s"/> Third, cluster database size for etcd.

Alex: <break time="0.3s"/> What can the four X L tier do?

Jordan: <break time="0.5s"/> Four X L supports up to sixty-eight hundred concurrent API requests and four hundred pods scheduled per second. <break time="0.4s"/> In stress testing, AWS showed it can support forty thousand nodes and six hundred forty thousand pods. <break time="0.3s"/> That's an eight X improvement over standard control planes.

Alex: <break time="0.3s"/> What does it cost?

Jordan: <break time="0.4s"/> X L starts at one dollar sixty-five per hour. <break time="0.5s"/> Four X L is six dollars ninety per hour. <break time="0.3s"/> That's in addition to standard or extended support hourly charges. <break time="0.4s"/> You can switch tiers as workloads change, or revert to standard control plane during quieter periods.

Alex: <break time="0.5s"/> Now let's shift to cloud operations because AWS made equally significant announcements there. <break time="0.4s"/> Starting with CloudWatch.

Jordan: <break time="0.4s"/> CloudWatch now has comprehensive observability for generative AI applications and agents. <break time="0.5s"/> You get built-in insights into latency, token usage, and errors across your AI stack. <break time="0.3s"/> No custom instrumentation required.

Alex: <break time="0.3s"/> What frameworks does it support?

Jordan: <break time="0.4s"/> It works with Amazon Bedrock AgentCore natively. <break time="0.5s"/> But it's also compatible with open-source agentic frameworks like LangChain, LangGraph, and CrewAI. <break time="0.3s"/> You can monitor model invocations, trace agent workflows end-to-end, and identify performance bottlenecks.

Alex: <break time="0.3s"/> This is critical as more teams deploy AI agents.

Jordan: <break time="0.5s"/> Exactly. <break time="0.3s"/> Agent observability has been a gap. <break time="0.4s"/> You deploy an agent, and when something goes wrong, you're debugging in the dark. <break time="0.3s"/> Now you have proper tracing and metrics out of the box.

Alex: <break time="0.4s"/> What about the DevOps Agent?

Jordan: <break time="0.5s"/> The AWS DevOps Agent is in preview, and it's essentially an autonomous on-call engineer. <break time="0.4s"/> It analyzes data across CloudWatch, GitHub, ServiceNow, and other tools to identify root causes and coordinate incident response.

Alex: <break time="0.3s"/> Has anyone measured the impact?

Jordan: <break time="0.4s"/> The Kindle team reported eighty percent time savings using CloudWatch Investigations, which is the underlying technology. <break time="0.5s"/> The DevOps Agent goes further by acting proactively. <break time="0.3s"/> It correlates telemetry, identifies root causes, and reduces operational burdens so teams can focus on higher-value work.

Alex: <break time="0.4s"/> CloudWatch also got M C P servers?

Jordan: <break time="0.4s"/> Yes. <break time="0.5s"/> The M C P servers for CloudWatch bridge AI assistants to your observability data. <break time="0.3s"/> They provide standardized access to metrics, logs, alarms, traces, and service health data. <break time="0.4s"/> You can build autonomous operational workflows and integrate CloudWatch with AI-powered development tools.

Alex: <break time="0.3s"/> What about data management?

Jordan: <break time="0.5s"/> CloudWatch introduced a unified data store for operations, security, and compliance data. <break time="0.3s"/> It automates collection from AWS and third-party sources like CrowdStrike, Microsoft Office three sixty-five, and SentinelOne. <break time="0.4s"/> Everything gets stored in S three Tables with O C S F and Apache Iceberg support.

Alex: <break time="0.3s"/> That simplifies the log aggregation problem.

Jordan: <break time="0.4s"/> Significantly. <break time="0.5s"/> Automatic normalization across sources, native analytics integration, built-in support for industry-standard formats. <break time="0.3s"/> And the first copy of centralized logs incurs no additional ingestion charges. <break time="0.4s"/> It's actually cost-efficient for multi-account log management.

Alex: <break time="0.3s"/> What else is new in observability?

Jordan: <break time="0.5s"/> Application Signals now has a GitHub Action that provides observability insights during pull requests and CI/CD pipelines. <break time="0.4s"/> Developers can identify performance regressions without leaving their development environment. <break time="0.3s"/> There's also auto-discovery that visualizes application topology without requiring instrumentation.

Alex: <break time="0.3s"/> And database insights?

Jordan: <break time="0.4s"/> CloudWatch Database Insights now supports cross-account and cross-region monitoring. <break time="0.5s"/> You can get centralized visibility into database performance across your entire AWS organization. <break time="0.3s"/> It supports R D S, Aurora, and DynamoDB from a single monitoring account.

Alex: <break time="0.5s"/> Let's wrap up with what all this means for platform engineers.

Jordan: <break time="0.4s"/> There are four big takeaways. <break time="0.5s"/> First, scale is no longer a constraint. <break time="0.3s"/> One hundred thousand nodes opens new architecture patterns that simply weren't possible before.

Alex: <break time="0.3s"/> Second?

Jordan: <break time="0.4s"/> GitOps becomes turnkey. <break time="0.5s"/> E K S Capabilities eliminates the maintenance burden of running Argo C D and A C K. <break time="0.3s"/> That's real operational toil that goes away.

Alex: <break time="0.3s"/> Third?

Jordan: <break time="0.4s"/> Natural language ops is real. <break time="0.5s"/> The M C P Server changes who can operate clusters. <break time="0.3s"/> We're moving from kubectl mastery to conversational cluster management.

Alex: <break time="0.3s"/> And fourth?

Jordan: <break time="0.5s"/> Observability goes proactive. <break time="0.3s"/> DevOps Agent as your first line of defense, gen AI tracing out of the box, unified data management. <break time="0.4s"/> The operations experience is fundamentally changing.

Alex: <break time="0.4s"/> What's the overall pattern here?

Jordan: <break time="0.5s"/> E K S is evolving from a container orchestration service into a fully managed AI cloud platform. <break time="0.4s"/> AWS is betting that Kubernetes will anchor the next decade of AI infrastructure. <break time="0.3s"/> Every announcement we covered today focuses on AI workload enablement.

Alex: <break time="0.4s"/> Next episode, we're wrapping up our re:Invent coverage with part four. <break time="0.3s"/> Data and AI services. <break time="0.4s"/> S three Tables, Zero E T L, and how AWS is rethinking the data stack for AI workloads.

Jordan: <break time="0.5s"/> The infrastructure announcements we covered today set the foundation. <break time="0.4s"/> For platform engineers, the question isn't whether to adopt these capabilities. <break time="0.3s"/> It's how fast you can integrate them into your platform strategy. <break time="0.4s"/> Because the teams that master AI infrastructure will define the next era of software delivery.
