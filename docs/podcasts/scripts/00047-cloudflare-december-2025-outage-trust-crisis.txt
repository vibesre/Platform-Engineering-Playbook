Jordan: Today we're tackling something that's hitting close to home for a lot of platform teams right now. <break time="0.3s"/> But first, let's check the pulse of platform engineering.

Jordan: <break time="0.5s"/> KubeCon surveys are in, and the data is fascinating. The New Stack polled two hundred nineteen platform teams at KubeCon, and the results show AI adoption in platform engineering is accelerating fast. We're seeing teams integrate AI into everything from incident response to capacity planning. Worth reading if you're building your twenty twenty-six roadmap.

Jordan: GitHub Actions got a quality-of-life upgrade. Workflow dispatch now supports twenty-five inputs, up from just ten. If you've ever hit that limit trying to parameterize complex deployment pipelines, you know how painful that ceiling was.

Jordan: And Louis Ryan from Google gave a presentation on hybrid cloud-native networking that's getting a lot of attention. He critiques what he calls the "big IP problem" <break time="0.2s"/> and rigid security policies that become single points of failure. Relevant to today's main topic, actually.

Jordan: <break time="0.5s"/> Speaking of which, let's talk about the elephant in the room. Cloudflare went down again this morning. <break time="0.3s"/> And if you're having déjà vu, <break time="0.2s"/> you should be.

Alex: Three weeks. <break time="0.4s"/> That's how long it took for Cloudflare to have another major outage. December fifth, twenty twenty-five, eight forty-seven <phoneme alphabet="ipa" ph="juː.tiː.siː">UTC</phoneme>, twenty-eight percent of HTTP traffic goes down. <break time="0.3s"/> LinkedIn, Zoom, Fortnite, ChatGPT, Shopify, Coinbase. <break time="0.3s"/> Sound familiar?

Jordan: We covered their last outage in Episode thirty just three weeks ago. That one was the Rust panic in their bot management system that took down twenty percent of the internet for over four hours. This time it was twenty-five minutes, but the pattern is what concerns me.

Alex: Let's break down what happened this morning. Cloudflare was deploying a killswitch to disable a rule in their web application firewall. This was part of mitigating the React Server Components <phoneme alphabet="ipa" ph="siː.viː.iː">CVE</phoneme> that hit the industry this week.

Jordan: So they were trying to protect customers from a security vulnerability.

Alex: Exactly. And in doing so, they triggered a <phoneme alphabet="ipa" ph="ˈluːə">Lua</phoneme> code bug that had existed undetected for years. When the killswitch disabled a rule with an "execute" action type, the code tried to access an object that wasn't there. Classic nil value access.

Jordan: The error message was literally "attempt to index field execute, a nil value." <break time="0.2s"/> Line three fourteen of their init dot <phoneme alphabet="ipa" ph="ˈluːə">lua</phoneme> file.

Alex: The thing is, this code path had never been exercised before. They'd never applied a killswitch to this specific rule type. <phoneme alphabet="ipa" ph="ˈluːə">Lua</phoneme> doesn't have strong type checking, so the bug just sat there, dormant, waiting for the exact conditions to trigger it.

Jordan: Walk me through the timeline.

Alex: Eight forty-seven <phoneme alphabet="ipa" ph="juː.tiː.siː">UTC</phoneme>, the bad configuration deploys. Eight forty-eight, it's propagated across the entire network. <break time="0.2s"/> No gradual rollout, just instant global deployment. Eight fifty, automated alerts fire. Nine eleven, they initiate the rollback. Nine twelve, full restoration.

Jordan: Twenty-five minutes total. <break time="0.3s"/> But two minutes for alerts to fire after global propagation?

Alex: That's one of the things the community is criticizing. Two minutes might not sound like much, but when you're talking about twenty-eight percent of HTTP traffic, <break time="0.2s"/> that's millions of failed requests per second.

Jordan: <break time="0.5s"/> And this is where the pattern becomes impossible to ignore. Let me run through Cloudflare's twenty twenty-five track record.

Alex: Please do.

Jordan: March twenty-first, one hour seven minutes down. R2 credential rotation error. <break time="0.3s"/> June twelfth, two hours twenty-eight minutes. Workers <phoneme alphabet="ipa" ph="keɪ.viː">KV</phoneme>, Access, and WARP degraded. <break time="0.3s"/> July fourteenth, sixty-two minutes. Their one dot one dot one dot one DNS resolver topology change gone wrong. <break time="0.3s"/> September, the dashboard and API outage that lasted hours. <break time="0.3s"/> November eighteenth, four-plus hours, the bot management Rust panic we covered in Episode thirty. <break time="0.3s"/> And now December fifth, twenty-five minutes, this <phoneme alphabet="ipa" ph="ˈluːə">Lua</phoneme> killswitch bug.

Alex: That's six major outages in nine months <break time="0.2s"/> for a company that handles twenty percent of all internet traffic.

Jordan: The community reaction has been... <break time="0.3s"/> pointed. On Hacker News, someone calculated that Cloudflare is now below ninety-nine point nine percent uptime for the year.

Alex: Which, to be fair, Cloudflare would probably dispute based on how they calculate their <phoneme alphabet="ipa" ph="ˌes.el.eɪz">SLAs</phoneme>. But the perception matters as much as the math.

Jordan: One of my favorite comments was <break time="0.2s"/> "Wanted to check if Cloudflare is down, went to Downdetector dot com... <break time="0.3s"/> Downdetector runs on Cloudflare too." <break time="0.2s"/> Even the outage tracking site was down.

Alex: The irony of a security fix causing an outage isn't lost on people either. Someone wrote <break time="0.2s"/> "the risk mitigation system became the systemic risk itself."

Jordan: <break time="0.5s"/> Let's talk about something that doesn't get enough attention in these postmortems. <break time="0.3s"/> The human cost.

Alex: This is where it gets real for me. We focus so much on the technical root cause, the timeline, the blast radius. But somewhere, there's an on-call engineer who got paged at three AM for something that wasn't their fault <break time="0.2s"/> and that they couldn't fix.

Jordan: The twenty twenty-four State of DevOps report found that sixty-seven percent of IT professionals experience burnout. And on-call rotations are a huge contributor. You're supposed to allocate thirty to forty percent of your work bandwidth during an on-call shift just for incident response.

Alex: But what happens when the incidents aren't yours to fix? <break time="0.3s"/> Your app is down. Your customers are angry. Your status page shows red. And you're sitting there watching Cloudflare's status page, <break time="0.2s"/> waiting for them to fix it.

Jordan: McKinsey found that one in four employees globally experience burnout symptoms. And I'd bet platform engineers dealing with infrastructure dependencies they can't control <break time="0.2s"/> are overrepresented in that statistic.

Alex: Think about what happens in those twenty-five minutes. Your monitoring lights up. PagerDuty goes off. You jump online, start investigating. Is it us? Check the code, check the deploys, check the database. <break time="0.2s"/> Nothing changed on our side.

Jordan: Then you start the external investigation. Is it AWS? Is it our DNS? Is it... <break time="0.3s"/> oh, Cloudflare is down. <break time="0.2s"/> Great.

Alex: And now you're in communications mode. Update the status page. Tell leadership. Respond to the angry Slack messages. Explain to customers that it's not your fault but you're working on it.

Jordan: "Working on it" <break time="0.2s"/> meaning "watching someone else's status page and hoping."

Alex: And when it comes back, you have to deal with the aftermath. The recovery traffic spike. The stuck requests. The cache invalidation. The explaining to leadership why your <phoneme alphabet="ipa" ph="ˌes.el.eɪz">SLAs</phoneme> are busted this month.

Jordan: The emotional toll is real. You're responsible for reliability, but you can't control the infrastructure you depend on. That mismatch between responsibility and control <break time="0.2s"/> is a recipe for burnout.

Alex: And it keeps happening. Two outages in three weeks. <break time="0.3s"/> How do you build a culture of reliability when your foundational dependencies are unreliable?

Jordan: <break time="0.5s"/> So what do we actually do about this?

Alex: The organizations that sailed through both the November and December outages had one thing in common. Multi-CDN strategies. They weren't dependent on a single provider.

Jordan: That's not always practical for smaller teams, but even basic redundancy helps. Can your critical paths fall back to direct connections if your CDN fails?

Alex: Synthetic monitoring is crucial, and it needs to be outside your CDN. If you're monitoring through Cloudflare and Cloudflare goes down, <break time="0.2s"/> your monitoring is blind.

Jordan: We've talked before about runbooks for internal incidents. But do you have a runbook for "major provider is down"? What's the communication plan? What can you actually do versus what do you just have to wait out?

Alex: And this is the one that I think gets overlooked. Your on-call wellness is a legitimate engineering concern. If your team is getting paged for things they can't control, that's not just frustrating, <break time="0.2s"/> it's a retention risk.

Jordan: Cloudflare, for their part, has promised improvements. Enhanced gradual rollouts with health validation. Fail-open error handling instead of hard crashes. They say they'll publish detailed resiliency projects before the end of next week.

Alex: And we'll see. The November postmortem made similar promises, <break time="0.2s"/> and here we are three weeks later.

Jordan: The pattern I'm seeing isn't just technical. It's organizational complexity outpacing operational safeguards. Cloudflare has grown enormously. Their infrastructure is incredibly sophisticated. But sophistication creates more failure modes, and their processes haven't kept pace.

Alex: That's a lesson every platform team should internalize. As you add complexity, your operational maturity needs to scale with it. Otherwise, you're just adding more places where latent bugs can hide.

Jordan: <break time="0.5s"/> Let's bring it back to that number. Three weeks between major outages. Twenty percent of the internet affected. Sixty-seven percent burnout rate in our industry. <break time="0.3s"/> These aren't just statistics. They're the environment we're building in.

Alex: Your infrastructure strategy should assume your providers will fail. The question isn't if, it's when. And the real question is whether you've prepared your systems <break time="0.2s"/> and your people <break time="0.2s"/> for that reality.

Jordan: The infrastructure we depend on is only as reliable as its weakest deployment. Make sure your teams aren't paying the price for dependencies they can't control.

Alex: And maybe check that your status page doesn't run on Cloudflare.

Jordan: <break time="0.3s"/> The fundamentals haven't changed. Redundancy matters. Monitoring matters. But increasingly, the human element matters too. <break time="0.3s"/> Take care of your on-call teams. They're dealing with enough.
