Jordan: Welcome to part two of our four-part AWS re:Invent twenty twenty-five series. <break time="0.4s"/> In episode forty-nine, we covered agentic AI and frontier agents. Today, we're going deep on infrastructure: chips, serverless evolution, and Werner Vogels' framework for thriving in the AI era. <break time="0.6s"/> But first, let's check what else is happening in platform engineering.

Jordan: BellSoft unveiled their hardened container images at KubeCon twenty twenty-five. <break time="0.3s"/> Here's the sobering context: typical container images contain over six hundred known vulnerabilities. Nearly half of all Java services currently contain CVEs that attackers are actively exploiting in the wild. <break time="0.4s"/> BellSoft's solution? Hardened images with ninety-five percent fewer CVEs. They're built on Alpaquita Linux, stripped of package managers and non-essential components. Immutable configurations that prevent runtime tampering. Plus full software bill of materials for compliance auditing. <break time="0.3s"/> For platform teams running Java workloads, this is worth evaluating.

Alex: And here's a critical read for anyone building CI/CD platforms. <break time="0.3s"/> A deep analysis just dropped on why GitHub Actions might have quote, the worst package manager. End quote. <break time="0.4s"/> The core problem: no lockfiles. Every workflow run re-resolves dependencies, and results can change without any modification to your code. <break time="0.3s"/> Version tags like actions slash checkout at v4 are mutable. Maintainers can retag without warning, silently altering your workflow behavior. <break time="0.4s"/> Research found fifty-four percent of composite actions contain at least one security weakness, mostly from invisible transitive dependencies. <break time="0.3s"/> No checksums validate downloaded code. And failed job re-runs might execute completely different code if force-pushes happened in between. <break time="0.4s"/> If your platform relies on GitHub Actions for trusted publishing to package registries, this cascade of vulnerabilities should concern you.

Jordan: Speaking of infrastructure decisions, Proxmox just released Datacenter Manager one point zero stable. <break time="0.3s"/> This is positioned as a VMware escape hatch. Centralized management across multiple Proxmox environments, VM migration between clusters without manual network reconfiguration, and fleet management tools for patching and updates. <break time="0.4s"/> It's built in Rust, runs on Debian Trixie with Linux kernel six point seventeen. For organizations looking to exit VMware after the Broadcom acquisition, this opens the floodgates. <break time="0.5s"/> Now let's dive into AWS re:Invent. The infrastructure announcements were substantial.

Alex: Let's start with silicon. <break time="0.3s"/> AWS is building its own chip empire, and Graviton five is the latest milestone.

Jordan: Why does custom silicon matter so much?

Alex: Control and differentiation. <break time="0.3s"/> When you design your own chips, you control the cost structure. You can optimize for your specific workloads. And you create features competitors can't easily replicate. <break time="0.4s"/> Here's the key stat: ninety-eight percent of AWS's top one thousand customers are already using Graviton. This isn't experimental anymore. It's mainstream infrastructure. <break time="0.3s"/> And here's the business case: AWS claims up to forty percent better price-performance versus x86 for comparable workloads. That's margin you're leaving on the table if you haven't migrated.

Jordan: What does Graviton five actually deliver?

Alex: One hundred ninety-two cores. <break time="0.3s"/> That's three times more cores than Graviton four's sixty-four. <break time="0.4s"/> Twenty-five percent better compute performance per core. Thirty-three percent lower inter-core latency. And a five-times larger L3 cache for memory-bound workloads. <break time="0.4s"/> The memory bandwidth is the sleeper feature. They're claiming over two hundred fifty million operations per second for in-memory workloads like Redis and Memcached. <break time="0.3s"/> It's built on Arm Neoverse V3 architecture using TSMC's three nanometer process. The new instance types, M9g, C9g, and R9g, launch in twenty twenty-six.

Jordan: What are customers actually seeing in production?

Alex: SAP reported thirty-five to sixty percent performance improvement for S/4HANA workloads. <break time="0.3s"/> Atlassian saw thirty percent higher performance with significant cost reduction. <break time="0.3s"/> Honeycomb, the observability company, measured thirty-six percent better throughput. <break time="0.4s"/> These aren't benchmarks. These are production results.

Jordan: So for platform teams, the question is whether their workloads are Graviton-ready?

Alex: Exactly. Most container workloads compile seamlessly for ARM. The migration patterns are well-established now. If you're not running Graviton, you're leaving price-performance on the table.

Jordan: <break time="0.6s"/> Let's talk about AI training economics. Trainium three is AWS's answer to GPU constraints.

Alex: The challenge is that training large models is expensive. GPU supply is still constrained. And NVIDIA's pricing power is substantial. <break time="0.3s"/> AWS is offering an alternative: custom AI chips designed specifically for training workloads. <break time="0.4s"/> The strategic play here is vertical integration. AWS wants to be the most cost-effective place to train foundation models, and custom silicon is how they get there.

Jordan: What do the numbers look like?

Alex: Trainium three UltraServers deliver four point four times more compute performance versus Trainium two. <break time="0.3s"/> Fifty percent cost reduction for AI training. That's the headline. <break time="0.4s"/> Three hundred sixty-two FP8 petaflops per UltraServer with up to one hundred forty-four chips. <break time="0.3s"/> Four times better energy efficiency, which matters for data center economics. <break time="0.4s"/> But here's what's architecturally interesting: NeuronLink, their custom interconnect, eliminates fifty percent of network overhead for distributed training. <break time="0.3s"/> EC2 UltraClusters three point zero can connect thousands of UltraServers scaling up to one million chips total. <break time="0.4s"/> That's designed for training frontier models at scale.

Jordan: Who's actually using this?

Alex: Anthropic is using Trainium for Claude training. <break time="0.3s"/> Metagenomi for genomics research. <break time="0.3s"/> Ricoh for document processing AI. <break time="0.3s"/> And Decart achieved four times faster inference for real-time generative video at half the cost of GPUs. <break time="0.4s"/> AWS also announced Trainium four on the roadmap, which will be NVIDIA NVLink compatible. They're playing the long game.

Jordan: And there's an on-premises option now?

Alex: AWS AI Factories. Complete Trainium-based training infrastructure for organizations with data sovereignty requirements. You can run AWS's AI compute stack in your own data center.

Jordan: <break time="0.6s"/> Now let's talk about Lambda. AWS just fundamentally changed what serverless can do.

Alex: Lambda Durable Functions. <break time="0.3s"/> The traditional Lambda timeout is fifteen minutes. Complex workflows required Step Functions. <break time="0.4s"/> Now you can build stateful workflows directly in Lambda that run from seconds to one full year.

Jordan: How does it work?

Alex: Two new primitives. <break time="0.3s"/> context dot step creates durable checkpoints. Your function can execute some code, checkpoint the result, and if anything fails, it resumes from that checkpoint. <break time="0.4s"/> context dot wait suspends execution and resumes when an event arrives. You can wait for human approval. External API callbacks. Timer expirations. All natively in Lambda.

Jordan: Can you give us an example?

Alex: Picture a data pipeline that fetches data, then waits up to seven days for human approval, then processes the data after approval. <break time="0.4s"/> In the old world, you'd build a Step Functions state machine, handle the callback pattern, manage the state store. <break time="0.3s"/> Now it's three lines of code with context dot step and context dot wait.

Jordan: What use cases does this unlock?

Alex: Human approval workflows. Long-running data pipelines. Multi-day batch processing. Event-driven orchestration. <break time="0.4s"/> If you're using Step Functions for straightforward state management, Lambda Durable might be simpler. It's not replacing Step Functions for complex orchestration, but it eliminates a lot of boilerplate.

Jordan: What else is new in Lambda?

Alex: Lambda Managed Instances gives you EC2 compute power with Lambda's simplicity. It's a bridge between serverless and traditional compute. <break time="0.3s"/> And IPv6 VPC support saves you thirty-plus dollars per month on NAT Gateway costs. That's low-hanging fruit for any VPC running Lambda.

Jordan: <break time="0.6s"/> Let's talk cost optimization. Database Savings Plans.

Alex: Up to thirty-five percent savings for serverless databases. <break time="0.3s"/> Up to twenty percent for provisioned instances. <break time="0.3s"/> One-year term commitment. Covers Aurora, RDS, DynamoDB, and more.

Jordan: How does it work in practice?

Alex: You commit to a dollar-per-hour spend. AWS automatically applies that commitment across all covered databases. It's flexible across database types. For most tiers, there's no upfront payment required.

Jordan: Platform engineering angle?

Alex: This is an easy cost optimization lever. If your database spend is stable and predictable, commit today. Stack it with Reserved Instances where applicable. <break time="0.3s"/> The ROI calculation is straightforward: stable spend equals immediate savings.

Jordan: <break time="0.6s"/> Now let's talk about Werner Vogels. His final re:Invent keynote after fourteen years.

Alex: He opened by saying, <break time="0.3s"/> quote, this is my final re:Invent keynote. I'm not leaving Amazon or anything like that, but I think that after fourteen re:Invents you guys are owed young, fresh, new voices. End quote. <break time="0.4s"/> Then he laid out a framework for thriving in the AI era.

Jordan: The Renaissance Developer.

Alex: Five qualities. <break time="0.4s"/> First: be curious. AI lowers the barrier to learning new things. You can explore any technology in hours, not months. <break time="0.4s"/> Second: think in systems. Architecture matters more than ever. AI writes code. You design systems. <break time="0.4s"/> Third: communicate precisely. AI amplifies unclear thinking. Vague prompts produce vague code. <break time="0.4s"/> Fourth: own your work. Quote, vibe coding is fine, but only if you pay close attention to what is being built. The work is yours, not that of the tools. You build it, you own it. End quote. <break time="0.4s"/> Fifth: become a polymath. Cross-disciplinary skills differentiate. Breadth plus depth equals competitive advantage.

Jordan: And he introduced a concept every platform engineer needs to understand.

Alex: Verification debt. <break time="0.4s"/> AI generates code faster than humans can comprehend it. That creates a gap between what gets written and what gets understood. <break time="0.3s"/> The gap keeps growing until something breaks in production.

Jordan: So code reviews become more important, not less.

Alex: Exactly. Vogels said, <break time="0.3s"/> quote, we all hate code reviews. It's like being a twelve-year-old and standing in front of the class. But the review becomes the control point to restore balance. End quote. <break time="0.4s"/> For AI-generated code, human review is the last line of defense.

Jordan: The big question everyone asks: will AI take my job?

Alex: Vogels' answer: <break time="0.3s"/> quote, will AI take my job? Maybe. Will AI make me obsolete? Absolutely not. <break time="0.3s"/> If you evolve. End quote.

Jordan: <break time="0.6s"/> Let's wrap with key takeaways. <break time="0.4s"/> First, Graviton five is the new default. One hundred ninety-two cores, twenty-five percent faster, and ninety-eight percent of top customers already on Graviton. The ARM migration is no longer optional.

Alex: Second, Trainium three changes AI economics. Fifty percent cost reduction for training. If you're evaluating AI infrastructure, Trainium is now a serious alternative to NVIDIA.

Jordan: Third, Lambda Durable Functions simplify orchestration. Workflows that run up to a year. No more Step Functions for straightforward state management.

Alex: Fourth, Database Savings Plans are easy wins. Thirty-five percent savings on serverless databases. If your database spend is predictable, commit today.

Jordan: Fifth, verification debt is real. Werner Vogels' warning: AI speed creates new risks. Code reviews are more important, not less.

Alex: <break time="0.5s"/> Next episode, we're diving into EKS Capabilities. AWS is now managing your Argo CD, Crossplane, and ACK controllers. Combined with one hundred thousand node clusters and natural language Kubernetes management, is this the era of invisible infrastructure?

Jordan: The infrastructure decisions you make today compound for years. <break time="0.4s"/> Choose wisely.
