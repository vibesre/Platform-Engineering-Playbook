Let's build on what you've learned. Pause and try to remember: In Episode 3, how does Aurora replicate across regions? And can both regions accept writes?

Take a second.

Aurora: Storage-layer replication, 45 to 85 milliseconds lag, active-passive architecture. Only the primary region accepts writes. The secondary is read-only until you promote it.

This works great for hot-warm. Primary handles all writes, secondary stands ready for failover. But what if you need hot-hot? What if you need US-EAST-1 AND US-WEST-2 both accepting writes simultaneously? Users in both regions writing at the same moment?

Aurora can't do this. The primary writer lives in one region. You cannot write to the secondary without promoting it first. And promotion takes ninety seconds. That's not hot-hot.

Today you're learning about DynamoDB Global Tables - active-active replication where every region accepts writes. This is fundamentally different from everything we've covered so far.

By the end of this lesson, you'll understand how DynamoDB achieves multi-region writes, how to handle conflicts when two regions write to the same item at the same moment, and most importantly - when this is worth the cost and complexity.

What you'll master today: How DynamoDB Global Tables achieve active-active replication with sub-second lag. Conflict resolution strategies when two regions write simultaneously to the same data. When to use DynamoDB over Aurora - and critically, when NOT to. And the hidden costs that can hit twenty-five thousand dollars monthly for high-traffic tables.

Let's start with the fundamental difference between what we've learned and what we're about to explore.

Aurora: One writer, multiple readers. Active-passive.
DynamoDB Global Tables: Multiple writers, multiple readers. Active-active.

This changes everything.

Picture a global gaming application. Players in US-EAST-1 and EU-WEST-1 simultaneously. A player in New York updates their score. A player in London updates their inventory. Both writes happen at the exact same moment to different items in the same table.

With Aurora Global Database? You'd need to route all writes to US-EAST-1. The London player's write has to travel across the Atlantic, hit the US-EAST-1 Aurora primary, then replicate back to EU-WEST-1. That's 150 to 200 milliseconds round trip. For a real-time game, that's noticeable lag. Players feel it. Complaints roll in.

With DynamoDB Global Tables? The London player writes to EU-WEST-1. The New York player writes to US-EAST-1. Both writes succeed locally in ten to twenty milliseconds. Then DynamoDB replicates asynchronously in the background. Sub-second replication, typically one to three seconds later, both regions have both updates.

This is the power of active-active. And it's exactly what hot-hot patterns need.

Here's how it actually works under the hood. You create a DynamoDB table in your primary region. Enable Global Tables. Add replica regions - US-WEST-2, EU-WEST-1, wherever you need presence. Every region becomes a full replica with read and write capability.

Writes go to the local region. Fast. Low latency. DynamoDB replicates to other regions asynchronously. There's no primary, no secondary distinction. All regions are equal. All can accept writes.

The replication lag is typically one to three seconds. Not the 45 to 85 milliseconds you get with Aurora, but acceptable for most workloads. The trade-off is worth it - you get multi-region writes.

Why is replication slower than Aurora? Aurora ships minimal redo logs at the storage layer. Tiny. Efficient. DynamoDB replicates full items at the application layer. Bigger payloads, more processing. But you get the capability Aurora cannot provide - true active-active writes.

Now here's where this gets tricky. And this is where most teams mess up.

User updates their profile in US-EAST-1. Sets email to 'new at email dot com'. Same user, same exact moment, updates their profile in US-WEST-2. Sets email to 'different at email dot com'.

Both writes succeed locally. Both regions think they have the correct value.

Now DynamoDB replicates. US-EAST-1 sends 'new at email dot com' to US-WEST-2. US-WEST-2 sends 'different at email dot com' to US-EAST-1.

Which value wins?

This is the conflict resolution problem. Get it wrong, and you have data corruption. Silent, subtle, hard-to-debug data corruption that manifests weeks later in production.

DynamoDB's default solution: Last-writer-wins. Timestamp-based conflict resolution. Each write gets a timestamp. When conflicts occur, the write with the latest timestamp wins.

In our example: If the US-EAST-1 write happened at ten zero zero zero point one zero zero seconds and the US-WEST-2 write happened at ten zero zero zero point one five zero seconds, US-WEST-2 wins. 'Different at email dot com' becomes the final value in both regions.

Advantages are clear. Automatic. No custom logic required. Deterministic - the same conflict always resolves the same way. No application changes needed. DynamoDB handles it.

But the limitations will bite you. Clock skew between regions can cause the wrong winner. AWS synchronizes clocks via NTP, but they can drift. If the US-EAST-1 clock is two seconds ahead of the US-WEST-2 clock, US-EAST-1 writes always win even if they happened second. You get lost updates. One write gets silently discarded. And there's no way to preserve both values.

I've seen this cause production issues. User updates their address in one region. Updates their phone number in another region. Last-writer-wins means one update disappears. The user sees their address change but their phone number didn't update. They try again. Same problem. Support tickets pile up. Nobody understands why.

The better approach: Design your data model to avoid conflicts in the first place.

Strategy one: Partition by region. User data lives in their home region. Users don't roam between regions mid-session. No conflicts because each user only writes to their home region. Simple. Effective.

Strategy two: Merge semantics. Instead of overwriting, merge. For a shopping cart, append items rather than replacing the entire cart. Use DynamoDB's atomic increment for counters. Conflicts become additions, not replacements.

Strategy three: Read-modify-write with version checks. Read the current version number. Modify the data. Write with the expected version. If the version changed between your read and write, retry. This detects conflicts and lets your application decide what to do. More complex, but gives you full control.

Here's a real production pattern that actually works. E-commerce with global customers. The user's home region is where they registered. Shopping cart updates go to the home region - sticky sessions from Episode 4 keep them there. Inventory and catalog replicate globally, but from the user's perspective, they're read-only. Backend systems update them. Orders write to the home region, then replicate globally for fulfillment tracking.

This design minimizes cross-region conflicts while still providing global access to data. The key is understanding your write patterns and designing around them.

Now let's talk about when you should use DynamoDB versus Aurora, because this decision has massive cost implications.

Use DynamoDB Global Tables when you need active-active writes across regions. That's hot-hot pattern from Episode 2. When your access patterns are key-value or single-item lookups - that's DynamoDB's strength. When you can tolerate eventual consistency - one to three second lag is acceptable for your use case. When you need unlimited scale - DynamoDB auto-scales to millions of requests per second. And when your data model fits NoSQL - denormalized, no complex joins.

Real use cases: Session stores where user sessions are read and written globally. Gaming leaderboards where players update scores from anywhere. IoT device state where devices send data from multiple regions. Shopping carts where users shop from any region. User preferences and settings - low-conflict profile data.

Use Aurora Global Database when you need a relational data model. Complex queries, joins, multi-table transactions. When you're okay with active-passive - hot-warm is sufficient for your RTO requirements. When you need strong consistency, not eventual. When your access patterns require SQL flexibility. When you want lower replication lag - 45 to 85 milliseconds versus one to three seconds matters for your workload.

Real use cases: Order management where transactional integrity is required. Financial records where strong consistency is critical. Inventory systems where ACID transactions are needed. User authentication with relational user, role, and permission data.

Here's where DynamoDB becomes a cost trap. Aurora pricing is predictable. Instance cost plus storage plus data transfer. You can calculate it upfront. DynamoDB pricing scales with traffic. Write Capacity Units and Read Capacity Units multiplied by the number of regions. High traffic means high costs.

Let me show you the math that surprises teams.

Application writes one thousand items per second. Reads five thousand items per second. Each item is one kilobyte. Three regions: US-EAST-1, US-WEST-2, EU-WEST-1.

Writes: one thousand Write Capacity Units per second in each region. Because all regions accept writes. Each write costs you in that region.

Reads: five thousand Read Capacity Units per second in each region. Users read from their local region.

Replication: one thousand Write Capacity Units times two. You're replicating to two other regions. That's two thousand additional Write Capacity Units per region just for replication.

Total per region: One thousand WCUs for application writes at point zero zero zero sixty-five dollars per WCU-hour equals four hundred seventy-five dollars per month. Two thousand WCUs for replication writes equals nine hundred fifty dollars per month. Five thousand RCUs for reads at point zero zero zero one three dollars per RCU-hour equals four hundred seventy-five dollars per month. Storage: one hundred gigabytes times twenty-five cents equals twenty-five dollars per month.

Per region cost: eighteen hundred seventy-five dollars per month. Three regions: five thousand six hundred twenty-five dollars per month.

Aurora equivalent for comparable scale? Five to eight thousand dollars per month. So DynamoDB Global Tables is actually in the same ballpark as Aurora for high-traffic workloads. But remember - Aurora gives you hot-warm. DynamoDB at this cost tier gives you hot-hot active-active capability. You're paying for a different architectural pattern, not necessarily more expensive, but different.

You're paying for auto-scaling. Zero administration. Active-active capability. Unlimited scale. These capabilities have value. But you need to know the cost upfront.

Let me show you optimization strategies because these costs add up fast.

Strategy one: On-demand versus provisioned capacity. On-demand pricing means you pay per request. No capacity planning. Good for unpredictable traffic. Provisioned capacity means you reserve capacity upfront and get discounts. Good for steady traffic. For Global Tables, provisioned can save fifty percent if your traffic is predictable.

Strategy two: Not all regions need to accept writes. Make some regions read-only to save on replication costs. Your application routes writes to primary regions, reads go to the local region. You still get global read access with lower replication overhead.

Strategy three: Use Time-To-Live for ephemeral data. Session data, temporary state - configure TTL to auto-delete after expiration. This reduces storage and replication costs. No manual cleanup required.

Strategy four: Right-size your items. Write Capacity Units are calculated as bytes divided by one kilobyte, rounded up. A 1.5 kilobyte item costs two WCUs. A 1.0 kilobyte item costs one WCU. Optimize item size to stay under one kilobyte boundaries. Split large items if needed. This can cut costs in half.

Common mistakes I see repeatedly.

Mistake one: Treating Global Tables like single-region DynamoDB. Teams assume strong consistency across regions. Reality is eventual consistency with one to three second lag. Fix: Design for eventual consistency. Use version checks. Show users "synchronizing across regions" messages if needed.

Mistake two: Not planning for conflicts. Teams use default last-writer-wins without understanding implications. Reality is silent data loss. Wrong values persisting. Support tickets piling up. Fix: Design your data model to minimize conflicts. Use merge semantics. Partition by region where possible.

Mistake three: Cost blindness. Teams enable Global Tables without calculating replication costs. Reality is sixty thousand dollar per month bills for high-traffic tables. CFO demands explanation. Engineering scrambles to optimize after the fact. Fix: Calculate Write Capacity Units including the replication multiplier before you build. Use provisioned capacity for steady workloads.

Mistake four: Using DynamoDB when Aurora would work. Teams choose DynamoDB for hot-warm patterns where they only need active-passive. Cost: Paying eight to ten times more for capability they don't use. Fix: Use Aurora for hot-warm. Use DynamoDB only when you actually need hot-hot active-active writes.

Before we wrap up, pause and answer these questions.

Question one: What's the key difference between Aurora Global Database and DynamoDB Global Tables in terms of write capability?

Question two: Your application writes five hundred items per second. Each item is two kilobytes. You have two regions. How many Write Capacity Units are needed per region?

Question three: When would you use DynamoDB over Aurora?

Take a moment.

Answers.

Question one: Aurora is active-passive. Only the primary region accepts writes. DynamoDB is active-active. All regions accept writes simultaneously. This is the fundamental architectural difference.

Question two: Each two kilobyte item needs two Write Capacity Units. That's bytes divided by one kilobyte, rounded up. Five hundred items per second times two WCUs equals one thousand WCUs for application writes. Plus five hundred WCUs for replication to the other region. Total per region: one thousand five hundred WCUs. At point zero zero zero sixty-five dollars per WCU-hour, that's approximately seven hundred dollars per month per region just for writes.

Question three: Use DynamoDB when you need hot-hot active-active writes, can tolerate eventual consistency, have key-value access patterns, and need unlimited auto-scaling. Don't use it if you need a relational model, strong consistency, or can get by with hot-warm. For equivalent traffic scale, DynamoDB and Aurora costs are actually comparable - the difference is architectural pattern, not raw cost.

Let's recap what we covered.

First: Active-active replication. DynamoDB Global Tables allow writes in all regions simultaneously. Aurora only allows writes in the primary region. This enables hot-hot patterns but comes with complexity.

Second: Conflict resolution. Default is last-writer-wins based on timestamp. Clock skew can cause wrong winners. Application-level resolution is better - design your data model to avoid conflicts in the first place.

Third: Replication lag is one to three seconds typical. Slower than Aurora's 45 to 85 milliseconds but acceptable for many use cases. The trade-off for active-active capability.

Fourth: Cost multiplier for replication. N minus one Write Capacity Units per region where N is region count. Three regions means two times replication cost on top of application writes. This adds up fast.

Fifth: Use cases matter. Hot-hot patterns, key-value workloads, eventual consistency acceptable - use DynamoDB. Hot-warm patterns, relational data, strong consistency - use Aurora. Choose the right pattern for your actual requirements, not for what sounds impressive.

Remember Episode 2? Hot-hot pattern with subsecond RTO? DynamoDB Global Tables is how you implement that for NoSQL workloads. It's not necessarily more expensive than Aurora - costs are similar for comparable scale - but you're paying for a different architectural pattern. The CCC Triangle from Episode 1 - you're trading complexity and operational burden for active-active capability.

Episodes 3 through 6 gave you the data layer options. Aurora Global Database: Active-passive hot-warm, relational with strong consistency, 45 to 85 millisecond lag, five to eight thousand per month for typical multi-region. DynamoDB Global Tables: Active-active hot-hot, NoSQL with eventual consistency, one to three second lag, five to ten thousand per month for comparable high-traffic workloads.

Choose based on your actual requirements. Not aspirational architecture. Not what sounds cool in architecture reviews. Your actual business needs, access patterns, and budget.

We'll revisit DynamoDB costs in Episode 9, Cost Management, with detailed optimization strategies. In Episode 10, Data Consistency Models, we'll explore the CAP theorem and why DynamoDB chose availability over consistency. And in Episode 15, Anti-Patterns, we'll see what happens when teams use Global Tables wrong - the production outages, the data corruption, the lessons learned.

Next time: Observability at Scale - centralized logging and distributed tracing.

You've got Aurora replicating data. DynamoDB replicating data. EKS clusters running workloads across regions. Network layer connecting everything. All of this is distributed. All of this can fail.

How do you troubleshoot when things break? When a request fails, which region was it in? Which service failed? How do you trace a request that touches three regions and five services? Where do you even start?

You'll learn centralized logging with CloudWatch Logs Insights and S3. How to aggregate logs from multiple regions without drowning in noise. Distributed tracing with X-Ray for multi-region request paths. How to follow a user request from US-EAST-1 through your API in US-WEST-2. Cross-region metrics aggregation and alerting. How to build dashboards that show your entire global architecture at a glance. And the observability patterns that prevented two AM pages - the real techniques production teams use.

Because multi-region without observability is flying blind. When production breaks at two AM, you need to know where and why. Fast. Before the outage costs you a hundred thousand dollars in lost revenue.

See you in Episode 7.
