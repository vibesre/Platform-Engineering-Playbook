Let's connect to what we covered. Pause and try to remember: In Episode 2, what was the five-minute RTO for hot-warm? And in Episode 3, what was Aurora's typical replication lag?

Take a second.

Five minutes RTO: one minute Aurora promotion, four minutes everything else. That 'everything else'? That's your Kubernetes layer. Aurora lag: forty-five to eighty-five milliseconds normal, one to thirty seconds under load.

Today you're learning how to implement that compute layer. Aurora gave you multi-region data. Kubernetes gives you multi-region compute. You need both for hot-warm.

By the end of this lesson, you'll understand how to architect EKS clusters across regions without the operational nightmare most teams create.

What you'll master today: Why the EKS control plane is your regional boundary and what that means for design. Independent clusters versus federated clusters - when to use each. Cross-cluster service discovery that actually works - hint, not Kubernetes federation. And the service mesh approach that Netflix and Spotify use in production.

Let's start with something most engineers miss: the EKS control plane is a regional service that depends on multiple availability zones. Not multi-region. Regional.

What does that mean? Your control plane - the brains of Kubernetes - lives in ONE region. US-EAST-1 or US-WEST-2. Not both.

Here's how it actually works. The EKS control plane is managed by AWS. It runs across three or more availability zones in a single region. The etcd cluster - that's your Kubernetes database - is distributed across those AZs using Raft consensus. Needs a two-of-three quorum to function. The API server is load balanced across AZs in that region. Your worker nodes can be in the same region, different AZs.

Why does this matter?

If your control plane is in US-EAST-1 and that region fails, you can't schedule new pods. You can't scale. You can't deploy. Your existing workloads keep running - the kubelet doesn't need the control plane for that. But you're in read-only mode.

This is fundamentally different from Aurora. Remember Episode 3? Aurora can failover its primary across regions. Storage layer replicates. Control plane promotes. Ninety seconds and you're operational in the secondary region.

EKS cannot do this. The control plane stays in its home region. It's tied to that region's infrastructure - the etcd cluster, the API servers, the networking. You cannot failover an EKS control plane across regions.

Picture this: two separate Kubernetes clusters. One control plane in US-EAST-1 managing worker nodes in US-EAST-1. Another control plane in US-WEST-2 managing worker nodes in US-WEST-2. They're independent. Not talking to each other. This is by design.

Here's the misconception engineers make: they think "multi-region Kubernetes" means one cluster spanning regions. Wrong. That's not how EKS works. You get multiple independent clusters, one per region. The control plane is your regional boundary. Accept this. Design around it.

Now let's talk about how those clusters relate to each other.

You've got two fundamental patterns: independent clusters and federated clusters.

Independent clusters - this is what I recommend. Two completely separate EKS clusters. One in US-EAST-1, one in US-WEST-2. Each has its own control plane managed by EKS. Its own worker nodes. Its own Deployments, Services, ConfigMaps. Its own monitoring and logging.

How do they coordinate? They don't coordinate at the Kubernetes level. They coordinate at the infrastructure level.

Aurora Global Database replicates data between regions - we covered that in Episode 3. DNS routes traffic using Route53 with health checks. Service mesh handles cross-cluster communication - we'll get to this. And your CI/CD deploys to both clusters. GitHub Actions, GitLab CI, whatever you use - it pushes the same manifests to both clusters.

What are the advantages? Simplicity. Each cluster operates independently. Resilient. One cluster failure doesn't affect the other. Flexible. You can run different versions for blue-green deployments. No blast radius. A problem in one region stays contained.

Remember Episode 1? The hidden SPOFs in Route53 and IAM that took down both regions during the October twenty-twenty-five outage? This is why you want independent clusters. If they're too tightly coupled, a control plane failure affects both. Keep them independent.

The alternative is federated clusters. And this mostly failed.

Kubernetes Federation was the official approach years ago. One "host cluster" managing multiple "member clusters." Automatically replicate resources across clusters. You deploy a Deployment to the host, and it magically appears in all member clusters.

It failed. Badly. Here's why.

Complexity explosion. Debugging across federated clusters is a nightmare. I've watched engineers spend eight hours - literally eight hours - chasing why a pod runs fine in US-EAST-1 but crashes immediately in US-WEST-2. Same Deployment manifest. Different node instance types? No. Different kernel versions? No. Different timing on secret propagation? Maybe. Eventually they found a race condition in their application startup - it was reading a secret that hadn't replicated yet. Eight hours for a thirty-second fix.

Eventual consistency issues. You update a ConfigMap in the host cluster. It takes thirty seconds to propagate to the member clusters. Your application in US-WEST-2 crashes because it's reading stale configuration. You're paged at two AM because production is on fire. You push a fix. Application recovers. But now you've got inconsistent state between clusters - nobody knows what configuration is actually running where. I've seen this cause cascading failures - one cluster's broken state affects monitoring, which affects alerts, which affects other clusters. It's distributed system chaos.

Limited adoption. Kubernetes Federation v1 was deprecated. Federation v2, called KubeFed, never reached maturity. The Kubernetes Special Interest Group couldn't solve the fundamental complexity problem.

And nobody uses it. Even Google - the creators of Kubernetes - doesn't recommend federation anymore. That should tell you something.

What replaced it?

Modern approaches: Multi-cluster service mesh like Istio or Linkerd. GitOps with Argo CD or Flux deploying to multiple clusters. Traffic management at the DNS or load balancer level. Service discovery via external DNS.

So when do you use what?

Use independent clusters when: You want operational simplicity. You can handle eventual consistency - and most applications can. You're implementing hot-warm or hot-cold patterns. You want blast radius isolation.

Consider federation alternatives - specifically service mesh - when: You need automatic failover at the service level. You have active-active traffic requirements. You're Netflix-scale with dedicated platform teams. And you can handle the operational complexity.

For most teams? Independent clusters with GitOps. Deploy the same manifests to both regions. Use DNS for traffic routing. Keep it simple.

Now here's the problem that trips up everyone: service discovery.

Your frontend pods in US-EAST-1 need to call your API service. Where is that API? In normal load, US-EAST-1. During failover, US-WEST-2. How do pods discover this?

In a single cluster, you'd use Kubernetes service discovery. The API service has a DNS name like api.default.svc.cluster.local. Pods resolve that, connect to the service, it load-balances to pods. Simple.

But that's cluster-local. It doesn't work across clusters. api.default.svc.cluster.local in US-EAST-1 only knows about pods in US-EAST-1. It has no idea about US-WEST-2.

Solution one: External DNS. This is the simplest approach.

Use DNS, not Kubernetes service discovery. Your API service is api.yourapp.com, not api.default.svc.cluster.local. Route53 health checks monitor US-EAST-1 API endpoints. On failure, Route53 updates DNS to point to US-WEST-2. Your pods resolve api.yourapp.com and get the current region.

How to implement: Each cluster exposes services via LoadBalancer or Ingress. External DNS controller watches Kubernetes services and creates Route53 records automatically. Health checks monitor each region's endpoints. DNS TTL is sixty seconds - that's the trade-off between failover speed and caching.

Pods use DNS names, not cluster-local service names. Your frontend doesn't connect to api.default.svc.cluster.local. It connects to api.yourapp.com.

Advantages? Simple. Works with any Kubernetes distribution. No special setup. Standard DNS resolution. You already know how DNS works.

Limitations? DNS caching. Sixty-second TTL means up to sixty seconds to detect failover. Pods might have longer cached values depending on their DNS client. No automatic retry logic - your application must handle DNS resolution. And Route53 failure affects all regions - remember Episode 1's hidden SPOFs?

Solution two: Service mesh. This is production grade but operationally heavier.

Istio, Linkerd, or Consul Connect. You install sidecar proxies on every pod. A service mesh control plane runs in each cluster. Multi-cluster mesh configuration connects the control planes. Services in US-EAST-1 can discover services in US-WEST-2 automatically.

Here's how it works. Install Istio in both clusters. Configure multi-cluster mesh with a shared root certificate - this is for mTLS security. Services in US-EAST-1 can now discover services in US-WEST-2 through the mesh. Envoy proxies - those sidecars - handle intelligent routing.

They prefer local endpoints. If your frontend in US-EAST-1 calls the API, Envoy routes to API pods in US-EAST-1 first. Lower latency, same cluster.

But if the API pods in US-EAST-1 start failing, Envoy detects this. Circuit breaker trips. It automatically fails over to API pods in US-WEST-2. No DNS wait. No application-level retry logic needed. The mesh handles it.

You get automatic retries, circuit breaking, timeout handling. And telemetry - you can see cross-cluster traffic flows in your observability tools.

Advantages? Automatic failover with no DNS delays. Intelligent routing that's latency-aware and locality-weighted. Observability with distributed tracing across clusters. Security with mTLS between all services.

Limitations? Complexity. Service mesh is operationally heavy. You're managing the mesh control plane, sidecar injections, certificate rotation, version upgrades. Performance overhead. Sidecar proxies add latency - typically two to five milliseconds per hop. Learning curve. Istio is notoriously complex. And resource cost. Every pod gets a sidecar proxy, that's more memory and CPU.

Let me show you a real production example.

E-commerce company running hot-warm. Primary cluster in US-EAST-1 serves ninety-five percent of traffic. Secondary cluster in US-WEST-2 at twenty percent capacity, ready to scale. They use Istio service mesh with multi-cluster configuration.

Frontend to API calls: prefer local, fail over to remote. During a US-EAST-1 failure - let's say the entire region goes down at 2 PM on Black Friday.

Envoy sidecars detect API failures in US-EAST-1. Health checks fail. Circuit breakers trip. Envoy automatically routes to US-WEST-2 API. No DNS wait. Traffic shifts immediately.

EKS auto-scaling in US-WEST-2 brings the cluster from twenty percent to full capacity. Pods schedule, scale up, ready in about two minutes.

Total failover time: two minutes. That's way faster than the five-minute DNS approach. But they're paying for it - operational complexity of running Istio, sidecar resource overhead, dedicated engineer just for mesh operations.

Is it worth it? For them, yes. They're processing two hundred thousand dollars per hour during peak. Two minutes versus five minutes saves them ten thousand dollars per outage. The Istio complexity cost is justified.

For a smaller company doing ten thousand per hour? Probably not worth it. DNS-based failover is fine.

Now let's talk about why Kubernetes Federation failed, because the lessons matter.

Federation promised automatic resource replication. Deploy once, runs everywhere. Reality was different.

Debugging hell. A pod fails in one cluster but not another. Why? Different node instance types? Different kernel versions? Different timing on secret propagation? You'd spend hours tracking down why the same manifest worked in one cluster and failed in another.

Eventual consistency nightmares. You update a ConfigMap in cluster A. It takes thirty seconds to propagate to cluster B. Your app crashes in cluster B because it's reading stale config. Then the update arrives. App recovers. But you've got thirty seconds of downtime and no clear indication of what went wrong.

Limited resource support. Federation v1 only supported basic resources - Deployments, Services. No CRDs. No StatefulSets initially. Your modern Kubernetes usage? Doesn't work with federation.

Community abandonment. The Kubernetes SIG Multi-Cluster gave up. They deprecated Federation v1. Federation v2 - KubeFed - never reached stable. The community couldn't solve the fundamental problem: managing multiple clusters as one is too complex.

What actually works? GitOps.

Argo CD or Flux. You have one Git repository with your Kubernetes manifests. Argo CD watches that repo. When you commit a change, Argo CD deploys to both US-EAST-1 and US-WEST-2 clusters. You control what goes where. Not automatic replication - explicit, declarative configuration per cluster.

If you need different configurations per cluster, you use Kustomize overlays or Helm values. Clear. Explicit. Debuggable.

Lesson learned: Don't try to make Kubernetes do multi-region at the control plane level. Keep clusters independent. Coordinate at the infrastructure and application level.

Let's talk about common mistakes, because I've seen these kill projects.

Mistake one: Trying to span regions with one cluster. Engineers say "let's just add worker nodes in US-WEST-2 to our US-EAST-1 control plane." It technically works in a lab. Pods schedule. Traffic flows. But then it hits production. Kubernetes API calls from US-WEST-2 pods take 150 milliseconds to reach the control plane in US-EAST-1. Your pod startup times double. Your deployments crawl. And you're paying for cross-region bandwidth on every API call - that's thousands of dollars per month. Then US-EAST-1 fails - maybe a region outage, maybe just the Availability Zone with your etcd quorum goes down. Your US-WEST-2 nodes are instantly orphaned. They can't talk to the control plane. Existing pods keep running, but you can't deploy anything. You can't scale. You're stuck for hours because failover to the actual secondary region takes time.

Fix: Accept the EKS boundary. Use independent clusters. One per region. Yes, it's more to manage. But it works.

Mistake two: Over-coupling clusters. I've seen teams implement "real-time" synchronization between clusters. Every ConfigMap, every Secret, every Service syncs instantly. Great idea in theory. In practice? One cluster has a bug that crashes its control plane. That controller is constantly syncing to the other region. It propagates the corruption. Both regions go down. Your RPO and RTO improve from "hours" to "catastrophic cascading failure across all regions."

Fix: Embrace eventual consistency. Coordinate only what's necessary. Most applications can handle configuration changes taking five to thirty seconds to propagate between regions. Use GitOps and declarative sync, not real-time replication.

Mistake three: Forgetting the control plane dependency creates a false sense of security. "We have multi-region Kubernetes, we're resilient." Here's the reality: During a region failure, yes, existing pods keep running in the secondary region. But you can't scale. Can't redeploy. Can't heal. A pod crashes in US-WEST-2 - the kubelet can't reach the control plane in US-EAST-1. There's no central orchestration. You're running orphaned containers. I've been on calls where teams believed they were resilient, then a region failed, and their "multi-region" setup was actually just machines running containers with nobody orchestrating. Not quite "multi-region" anymore.

Fix: Design for this reality. Use immutable, long-lived pods with proper health checks and resource limits. Or accept that during a control plane failure, you're degraded until you can failover to the secondary region.

Before we wrap up, pause and answer these questions.

Question one: Why can't you failover an EKS control plane across regions like you can with Aurora?

Question two: What's the main reason Kubernetes Federation failed?

Question three: You're implementing hot-warm. Do you use external DNS or service mesh for cross-cluster discovery? Think about the trade-offs.

Take a moment.

Answers.

Question one: EKS control plane is a regional service running in three AZs within one region. It depends on that region's infrastructure - etcd, API servers, networking. Aurora separates compute from storage. The storage layer can replicate across regions and promote. But EKS can't separate the control plane from its region. It's fundamentally tied to that region's availability zones.

Question two: Federation was too complex operationally. Debugging failures across clusters was impossible. You'd have the same manifest working in one cluster and failing in another with no clear reason why. Eventual consistency caused subtle bugs - ConfigMaps propagating late, applications crashing. The community couldn't solve the fundamental complexity problem. Even Google stopped recommending it.

Question three: Hot-warm with five-minute RTO? External DNS is simpler. Sixty-second DNS TTL is acceptable. Hot-warm with subsecond failover requirement? Service mesh, but only if you have the operational maturity to run Istio or Linkerd. Don't add service mesh complexity unless you need it.

Let's recap what we covered.

First: EKS control plane is regional, not multi-region. It runs in three availability zones within one region. This is your boundary. You cannot move it. Design around it.

Second: Independent clusters are the right pattern for most teams. One cluster per region. Coordinate at the infrastructure level - Aurora for data, DNS for traffic, CI/CD for deployments.

Third: Kubernetes Federation failed. Don't use it. Use GitOps with Argo CD or Flux. Or use service mesh for advanced use cases. But not federation.

Fourth: Cross-cluster service discovery has two approaches. External DNS for simplicity. Service mesh for automatic failover and intelligent routing. Choose based on your operational maturity and RTO requirements.

Fifth: Service mesh like Istio or Linkerd provides automatic failover, but adds operational complexity. Every pod gets a sidecar proxy. More memory, more CPU, more management overhead. Only use if you need the capabilities.

Remember Episode 2? Hot-warm pattern with five-minute RTO? Now you know how to implement the compute layer. Aurora Global Database from Episode 3: data layer, one-minute failover. EKS clusters from today: compute layer, two to four minutes to scale and route. Together: your five-minute RTO hot-warm architecture.

This independent cluster pattern is what we'll build on in Episode 11 when we cover advanced multi-region Kubernetes patterns. And in Episode 12, we'll walk through the actual failover procedure using these clusters.

Next time: Network Architecture. Transit Gateway, VPC Peering, PrivateLink.

You've got Aurora replicating data. You've got EKS clusters in multiple regions. How do they communicate?

You'll learn: When to use Transit Gateway versus VPC Peering, and why Transit Gateway scales. PrivateLink for secure cross-region service access. How to design network topology that doesn't create hidden bottlenecks. And the network layer mistakes that killed multi-region projects.

Because clusters don't talk to each other via magic. They talk via VPCs, subnets, and routing tables. Get this wrong, and your whole architecture breaks.

See you in Episode 5.
