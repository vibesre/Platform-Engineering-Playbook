Let's connect to what we've built. Pause and try to remember: In Episode 3, how does Aurora replicate data between regions? And in Episode 4, how do independent EKS clusters coordinate?

Take a second.

Aurora: Storage layer ships redo logs across regions. That 45 to 85 millisecond lag? Part of that is network latency. EKS: Independent clusters coordinate at infrastructure level - Aurora for data, DNS for traffic, CI/CD for deployments.

But here's what we haven't answered: HOW do they communicate? What network paths do Aurora redo logs actually take? When your frontend pod in US-EAST-1 calls an API in US-WEST-2, what happens at the network layer?

Today you're learning the network foundation. This is what everything else runs on. Get this wrong, and your entire multi-region architecture breaks. Get it right, and it's invisible.

By the end of this lesson, you'll know exactly how to connect your multi-region infrastructure without creating hidden bottlenecks or burning cash on unexpected data transfer charges.

What you'll master today: When to use Transit Gateway versus VPC Peering, and why Transit Gateway scales beyond a hundred VPC connections. How to design network topology that doesn't explode your route tables. PrivateLink for secure cross-region service access without exposing traffic to the internet. And the network layer mistakes that cost fifty thousand dollars monthly in surprise data transfer bills.

Let's start with the problem nobody explains clearly enough.

You've got Aurora in US-EAST-1 and US-WEST-2. You've got EKS clusters in both regions. They're running in separate VPCs. How do they talk?

They can't. Not yet. VPCs are isolated by default.

Your Aurora primary in the US-EAST-1 VPC can't send redo logs to Aurora secondary in the US-WEST-2 VPC. Your EKS pod in one region can't reach services in another region. Your monitoring system in one VPC can't scrape metrics from another VPC.

You have three options: Public internet - terrible idea. Security nightmare, high latency, unpredictable performance. VPC Peering - direct connection between two VPCs. Transit Gateway - hub-and-spoke model connecting many VPCs. Or PrivateLink - private service endpoints.

The choice determines your scalability, cost, and operational complexity. And most teams get this wrong initially.

Here's a real example. E-commerce company built hot-warm with VPC Peering. Started simple: four VPCs. Primary and secondary in two regions. Two peering connections. Easy to manage.

Two years later? Forty VPCs. Different business units. Multiple services. Separate environments for staging, testing, security scanning. The architecture evolved. The network design didn't.

VPC Peering requires N times N minus one divided by two connections. Forty VPCs equals seven hundred eighty peering connections. That's not a typo. Seven hundred eighty separate peering connections to manage.

Each peering connection adds routes to your route tables. And here's the limit that broke them: VPCs have a hundred twenty-five route table entry limit. Hard limit. You hit it, traffic stops routing. Services fail. Your multi-region architecture goes down.

They hit the limit during a normal deployment. Added one more VPC. Route tables maxed out. Production broke. Four-hour outage. Two hundred thousand dollar revenue loss.

Transit Gateway would have cost three hundred dollars per month. The outage cost two hundred thousand dollars. The math isn't even close.

Let's break down why VPC Peering looks simple but doesn't scale.

VPC Peering is a direct network connection between two VPCs. Point-to-point. Your US-EAST-1 VPC connects to your US-WEST-2 VPC. Traffic flows directly between them. Private IPs work. No internet gateway needed.

For small deployments, it's perfect. Same region or cross-region, doesn't matter. Traffic never touches public internet. Latency is low - about fifteen milliseconds cross-region for US-EAST-1 to US-WEST-2. Cost is straightforward: just data transfer at two cents per gigabyte.

The setup is dead simple. AWS console, a few clicks, done. No additional per-hour costs. For two to four VPCs, this is the right choice.

But watch what happens as you grow.

Two VPCs: one peering connection. Easy.
Four VPCs: six peering connections. Still manageable.
Ten VPCs: forty-five peering connections. Starting to get messy.
Twenty VPCs: one hundred ninety peering connections. Good luck managing that.
Forty VPCs: seven hundred eighty peering connections. This is where production breaks.

The formula is N times N minus one divided by two, where N is your VPC count. It's quadratic growth. And it kills you fast.

Each peering connection adds routes to your route tables. The VPC route table limit is one hundred twenty-five entries. You add routes for each peered VPC, plus your local routes, internet gateway, NAT gateway. You run out of space.

When you hit that limit, new routes fail. Traffic that should route to peered VPCs doesn't. Services can't communicate. Your applications break. And the error messages don't make it obvious - they just timeout or fail to connect.

Now let's talk about costs, because they're not insignificant. Data transfer across regions costs two cents per gigabyte. If Aurora replicates a hundred gigabytes per day, that's two dollars per day, sixty dollars per month. Not terrible.

But if you're replicating a terabyte per day? Twenty dollars per day. Six hundred dollars per month. Just for Aurora replication. Add in your application traffic, monitoring data, log shipping, backup replication. This adds up fast.

When do you use VPC Peering? Small deployments with four or fewer VPCs. Simple architectures - primary and secondary regions, minimal services. When you're cost-sensitive and can't justify per-hour charges. And critically, when you know your VPC count won't grow significantly.

If there's any chance you'll add VPCs as you grow - new business units, more services, additional environments - don't start with VPC Peering. You'll migrate to Transit Gateway later, and migrations are painful.

Now let's talk about Transit Gateway, because this is what actually scales.

Think of it like an airport hub. Instead of direct flights between every city - that's VPC Peering - you route through a central hub. All cities connect to the hub. The hub routes traffic between them.

Transit Gateway is that hub. All VPCs connect to Transit Gateway. Transit Gateway routes traffic between them.

Here's the scaling difference: N VPCs equals N connections. Not N squared. Forty VPCs? Forty connections. A hundred VPCs? A hundred connections. Linear scaling. No route table explosion.

Here's how it works in practice. You create a Transit Gateway in each region. You attach VPCs to the Transit Gateway. The Transit Gateway handles routing between VPCs automatically. You can peer Transit Gateways across regions. And you get centralized control of all your routing.

Picture your US-EAST-1 architecture. Transit Gateway connects your production VPC, staging VPC, monitoring VPC, and data VPC. Same structure in US-WEST-2. Then you create a Transit Gateway Peering connection between regions.

Cross-region traffic flows: VPC to Transit Gateway, Transit Gateway Peering to the other region's Transit Gateway, then to the destination VPC. Two hops through Transit Gateways, but the routing is automatic.

Advantages are significant. Scales linearly - N connections not N squared. Centralized routing control - one place to manage all routes. No route table limit issues - Transit Gateway handles routing internally. Supports transitive routing - VPC A can reach VPC C through VPC B. And it integrates with VPNs and Direct Connect.

But it's not free. Transit Gateway costs five cents per hour per attachment. That's thirty-six dollars per month per VPC attached. Ten VPCs attached equals three hundred sixty dollars per month base cost. Plus data transfer at two cents per gigabyte, same as VPC Peering.

Compare to VPC Peering: zero dollars per hour base cost, just data transfer. So where's the crossover point?

If you have more than three to four VPCs, Transit Gateway operational simplicity justifies the cost. You're paying for sanity. For not dealing with quadratic connection growth. For not hitting route table limits at 2 AM.

Performance is solid. Bandwidth up to fifty gigabps per VPC attachment. That's enough for most workloads. Latency adds one to two milliseconds compared to VPC Peering. For almost all use cases, that's negligible. And you get burst capacity - over a hundred gigabps aggregate across all attachments.

When do you use Transit Gateway? More than four to five VPCs. Growth expected - you're adding business units, services, environments. You need centralized routing control. Multi-region architecture with many services. Or you need transitive routing.

Let me show you a real production example. SaaS company running hot-warm architecture. US-EAST-1 has twelve VPCs: production, staging, per-service VPCs for microservices, monitoring, security scanning, data processing. US-WEST-2 mirrors that architecture. Twelve VPCs in each region.

They use Transit Gateway in each region. Transit Gateway Peering between regions. Twenty-four total attachments across both regions.

Cost breakdown: twenty-four attachments times thirty-six dollars equals eight hundred sixty-four dollars per month base. Data transfer: five hundred gigabytes per day times two cents times thirty days equals three hundred dollars per month. Total: one thousand one hundred sixty-four dollars monthly.

What would VPC Peering cost? Twelve VPCs in each region equals twenty-four VPCs total. That's two hundred seventy-six peering connections to manage. The route table management is a nightmare. They had three production outages in six months from routing misconfigurations before migrating to Transit Gateway.

They paid the eight hundred sixty-four dollars per month for operational sanity. It was absolutely worth it. No more 2 AM pages from routing failures. No more manual route table updates. No more hitting limits.

Now let's talk about PrivateLink, because it solves a different problem entirely.

PrivateLink lets you expose services privately without VPC Peering or Transit Gateway. The service provider creates a VPC endpoint service. The service consumer creates a VPC endpoint. Traffic stays on the AWS backbone. Never touches public internet. And critically, no VPC CIDR overlap issues.

Here's how it works. Service provider in US-EAST-1: you run your service behind a Network Load Balancer. You create a VPC Endpoint Service pointing to that NLB. You share the endpoint service name with consumers.

Service consumer in US-WEST-2: they create a VPC Endpoint in their VPC. They point it to your endpoint service name. They access your service via private DNS or IP. Traffic flows from consumer VPC through PrivateLink to your VPC. Secure, private, no internet exposure.

Use cases are specific. Expose APIs or services to other VPCs or regions without full VPC connectivity. Third-party integrations - share services with partner VPCs without giving them access to your entire network. Compliance requirements where you cannot expose traffic to the internet. Or SaaS service delivery - your product accessed by customer VPCs.

Advantages are compelling for service-oriented architectures. No CIDR overlap issues - each consumer gets a unique elastic network interface. Scales to thousands of consumers. Security isolation - consumers can't see each other. No route table management. Cross-region supported.

Limitations matter though. Requires Network Load Balancer at two cents per hour, about sixteen dollars per month. PrivateLink endpoint costs one cent per hour, seven dollars per month per consumer. Data transfer is one cent per gigabyte - cheaper than Transit Gateway or Peering across regions. But only TCP traffic works. No UDP, no ICMP.

Cost example: Monitoring service accessed by twenty VPCs across regions. Network Load Balancer: sixteen dollars per month. Twenty endpoints times seven dollars equals one hundred forty dollars per month. Data transfer: fifty gigabytes per day times one cent times thirty days equals fifteen dollars per month. Total: one hundred seventy-one dollars monthly.

Alternative with VPC Peering: twenty peering connections, routing complexity, no isolation between consumers. Alternative with Transit Gateway: twenty attachments times thirty-six dollars equals seven hundred twenty dollars per month.

For service-oriented architectures, PrivateLink wins on cost and operational simplicity.

Let me give you some design principles that actually matter in production.

First: Start simple. Use VPC Peering for your initial hot-warm setup. Primary plus secondary equals two VPCs per region equals four total. That's manageable. Add Transit Gateway when you hit five VPCs or foresee growth. Don't over-engineer early.

Second: Plan for data transfer costs. Cross-region bandwidth isn't free. Two cents per gigabyte adds up fast. One terabyte per month is twenty dollars. Ten terabytes is two hundred dollars. A hundred terabytes is two thousand dollars. One petabyte is twenty thousand dollars.

If Aurora replicates heavily, or your EKS services are chatty across regions, this hits your budget hard. Design to minimize cross-region traffic. Keep traffic local when possible. Remember Episode 4's service mesh and external DNS? They prefer local endpoints. Cross-region is failover only.

Third: Avoid routing table explosion. VPC route table limit is one hundred twenty-five entries. With VPC Peering, you need routes to every peered VPC. Forty peered VPCs equals forty routes, plus local, internet gateway, NAT gateway. You're approaching the limit. Transit Gateway needs one route to the Transit Gateway. Scales infinitely.

Let's talk about the mistakes that kill multi-region projects, because I've seen these repeatedly.

Mistake one: Using public internet for cross-region traffic. What: Engineers route cross-region traffic via public IPs to save money. Why it fails: Security exposure, high latency, unpredictable routing, compliance violations. Fix: Always use VPC Peering, Transit Gateway, or PrivateLink. The cost savings aren't worth the risk.

Mistake two: Ignoring data transfer costs. What: Architect the network without calculating cross-region bandwidth. Reality: Fifty thousand dollar per month surprise bills for high-traffic architectures. I've seen this happen. Company didn't calculate Aurora replication plus application traffic plus monitoring. Hit with a forty-five thousand dollar AWS bill at month-end. CFO was furious. Fix: Estimate traffic volumes upfront, calculate costs, design to minimize unnecessary cross-region transfers.

Mistake three: Hitting route table limits in production. What: Use VPC Peering for twenty-plus VPCs. Why it fails: Hit the one hundred twenty-five entry route table limit. Routes stop working. Services fail. Fix: Move to Transit Gateway before you hit ten VPCs. Plan for growth. The migration path from Peering to Transit Gateway is painful. Don't wait until you're forced to do it during an outage.

Mistake four: No bandwidth planning. What: Assume infinite cross-region bandwidth. Reality: Transit Gateway limited to fifty gigabps per attachment. If you're pushing high-volume data replication or real-time video processing, you can hit this. Fix: Calculate bandwidth needs, especially for burst traffic. Plan for peak, not average.

Before we wrap up, pause and answer these questions.

Question one: You have eight VPCs across two regions. Do you use VPC Peering or Transit Gateway? Why?

Question two: What's the formula for calculating VPC Peering connections?

Question three: Your Aurora replicates five hundred gigabytes per day cross-region. What's the monthly data transfer cost?

Take a moment.

Answers.

Question one: Transit Gateway. Eight VPCs is approaching the complexity threshold where VPC Peering becomes unmanageable. If you expect any growth - and you probably do - Transit Gateway scales linearly. Cost is eight times thirty-six equals two hundred eighty-eight dollars per month base. Worth it for operational simplicity and avoiding future route table nightmares.

Question two: N times N minus one, divided by two. Where N is the number of VPCs. Eight VPCs equals eight times seven divided by two equals twenty-eight peering connections. Already complex to manage. Gets exponentially worse as you add VPCs.

Question three: Five hundred gigabytes per day times thirty days equals fifteen terabytes per month. At two cents per gigabyte, that's three hundred dollars per month. Just for Aurora replication. This is why you design to minimize cross-region traffic when possible. Every gigabyte costs you.

Let's recap what we covered.

First: VPC Peering is simple and low-cost, but doesn't scale. Use for four or fewer VPCs. Growth pattern is quadratic. Route table limits will break you.

Second: Transit Gateway is hub-and-spoke that scales linearly. Use for five-plus VPCs or expected growth. Costs thirty-six dollars per month per VPC attachment. Worth it for sanity and avoiding operational nightmares.

Third: PrivateLink is for service-oriented architecture. Expose APIs privately without full VPC connectivity. Good for multi-consumer services. Cheaper than Transit Gateway for service endpoints.

Fourth: Data transfer costs are two cents per gigabyte cross-region. Calculate before you build. One terabyte per month is twenty dollars. A hundred terabytes per month is two thousand dollars. This adds up fast at scale.

Fifth: Route table limits are one hundred twenty-five entries per VPC. VPC Peering hits this with large deployments. Transit Gateway scales infinitely because routing is handled centrally.

Remember Episode 2? Hot-warm pattern with five-minute RTO? The network layer supports that. Aurora replication from Episode 3 travels over these networks. EKS cross-cluster communication from Episode 4 depends on this foundation. DNS failover needs network connectivity to reach services. Together: Data layer, compute layer, network layer equals your complete multi-region architecture.

We'll revisit Transit Gateway in Episode 8 when we cover DNS and Traffic Management - how Route53 integrates with your network topology. In Episode 9, Cost Management, we'll optimize these network costs. And in Episode 14, Security Architecture, we'll secure these network paths with encryption and private connectivity patterns.

Next time: DynamoDB Global Tables - active-active data replication.

Aurora gave you active-passive replication. One writer, one reader. What if you need active-active? Multiple regions accepting writes simultaneously?

You'll learn how DynamoDB Global Tables replicate with sub-second lag globally. Conflict resolution strategies when two regions write to the same item at the same moment. When to use DynamoDB over Aurora - and critically, when not to. And the hidden costs of global tables that can hit twenty-five thousand dollars monthly for high-traffic tables.

Because Aurora isn't the only answer. For certain workloads, DynamoDB's active-active replication is exactly what you need. But it comes with trade-offs most engineers don't expect.

See you in Episode 6.
