You've built multi-region architecture. Aurora, DynamoDB, EKS, networking, observability, DNS failover. It works perfectly in staging. Now comes the moment of truth: Actually failing over during a real outage at 2 AM without making things worse.

Here's what typically happens. Primary region fails. You wake up to pages. Half-asleep, you start executing failover. Skip a step in the runbook. Promote Aurora but forget to update DNS. Or update DNS but Aurora promotion fails. Or both work but secondary region doesn't have enough capacity. Outage extends from twenty minutes to four hours. Revenue loss multiplies. Post-mortem reveals basic mistakes.

Today you're learning disaster recovery procedures that actually work in production. Not the DR plan that sits in a Confluence document nobody reads. The actual runbooks engineers execute successfully at 2 AM under pressure.

By the end of this lesson, you'll understand how to test failover without affecting production. The step-by-step Aurora promotion procedure with validation at each stage. EKS traffic shifting with rollback safety. Rollback procedures when failover makes things worse. And chaos engineering to validate your DR actually works before you need it.

Let's start with testing DR without causing downtime, because your first failover shouldn't be during a real incident.

Most teams build multi-region, deploy it, never test it. Then primary region fails. They attempt failover for the first time ever. Discover health checks are misconfigured. Secondary Aurora doesn't have write permissions configured. Route53 failover policy has a typo. The DR "solution" doesn't actually work.

Testing strategy one: Weighted routing for traffic sampling. Before attempting full failover, use Route53 weighted routing to send one percent of traffic to secondary region. Episode 8's routing policies. Monitor error rates, latency, database write success. If secondary handles one percent perfectly, gradually increase to five percent, then ten percent. If errors spike, you found issues before full failover.

Real example: E-commerce site tested secondary region with one percent traffic. Discovered DynamoDB Global Tables hadn't fully replicated yet. Some items missing in secondary region. Users in that one percent saw "product not found" errors. Fixed replication issue before attempting failover. This test prevented what would have been a disastrous full cutover.

Testing strategy two: GameDay exercises in staging. Schedule quarterly DR drills. Simulate primary region failure by blocking traffic at network level. Execute full failover procedure in staging environment with production-like data. Time how long it takes. Document what breaks. Update runbooks based on findings.

Netflix does GameDay exercises monthly. They randomly terminate instances, kill services, simulate AZ failures, simulate region failures. Engineers practice recovery procedures regularly. When real failures happen, muscle memory kicks in. No panic, just execution.

Testing strategy three: AWS Fault Injection Simulator. Inject faults into production safely. Throttle Aurora to simulate performance degradation. Packet loss on cross-region connections. Terminate random EKS pods. Observe how systems handle failures. Validate monitoring alerts fire correctly. Confirm auto-scaling responds appropriately.

Start small. One pod termination. Then multiple pods. Then simulate database slow queries. Then network partition between regions. Gradually increase blast radius as confidence builds. This surfaces issues in controlled manner before real failures happen.

Now Aurora failover procedure step by step with validation gates.

Step one: Verify replication lag. Before promoting secondary, check Aurora replication lag is under five seconds. Episode 3 explained why this matters - lag above ten seconds means promoting will lose recent writes beyond RPO. CloudWatch metric ApproximateReplicationLag shows this. If lag is thirty seconds, wait for it to drop or accept data loss.

Step two: Stop writes to primary. Put application in read-only mode or route writes to a maintenance page. This prevents new writes during promotion that would be lost. Drain in-flight transactions, typically takes ten to twenty seconds. Application health endpoint returns degraded status, Route53 stops sending new traffic.

Step three: Promote secondary to primary. AWS Console or CLI command: aws rds promote-read-replica. This takes sixty to ninety seconds. Aurora reconfigures secondary from read-replica to standalone read-write instance. During this time, no writes are possible to either region. This is the service interruption window.

Step four: Verify promotion succeeded. Check Aurora instance status shows "available". Attempt test write to verify write capability. Don't enable application writes yet. Verify replication if you have additional read replicas.

Step five: Update Route53 to point to new primary. If using failover routing with health checks, this happens automatically when old primary health check fails. If manual, update A record to new primary endpoint. Wait sixty seconds for DNS propagation based on TTL from Episode 8.

Step six: Enable application writes. Change application from read-only mode to full operation. Monitor error rates carefully. Slowly ramp up write traffic to avoid overwhelming newly promoted instance. Watch for replication lag if you have other regions.

Step seven: Validate end-to-end. Execute test transactions. User signup, checkout flow, database writes. Confirm everything works. Check observability dashboards from Episode 7 showing healthy state.

Total time for planned failover: Two to five minutes. Ninety seconds Aurora promotion, sixty seconds DNS propagation, remaining time for validation and traffic ramp.

Unplanned failover is messier. Primary region completely offline. Can't gracefully drain traffic. Can't verify replication lag because primary is unreachable. Must accept potential data loss within RPO.

Unplanned procedure: Immediately promote secondary Aurora. Don't wait to check lag - primary is dead, you have no choice. Promotion takes ninety seconds. Update Route53 health checks if needed or they may auto-failover. Enable application writes immediately. Scale secondary EKS compute from twenty-five percent to one hundred percent capacity. This takes two to three minutes for auto-scaling. Monitor observability dashboards to verify secondary is handling full load. Accept that some recent writes may be lost - inform business stakeholders.

Real unplanned scenario: US-EAST-1 complete region failure at 6 AM. Primary Aurora unreachable. Team promoted US-WEST-2 Aurora immediately. Updated Route53 manually because health checks failed slowly. Scaled EKS from fifteen pods to sixty pods. Total recovery time: Four minutes. Data loss: Eight seconds of writes before failure. Business accepted this was within RPO. Users experienced four-minute outage instead of hours.

EKS traffic shifting procedure is separate from database failover.

For hot-warm with DNS failover: Route53 health checks mark primary unhealthy. Traffic automatically routes to secondary via failover policy. EKS pods in secondary region receive traffic. Auto-scaling triggers to handle increased load. Monitor pod health and scaling events. Typically takes two to three minutes for full traffic shift.

For hot-hot with service mesh from Episode 11: Envoy circuit breakers detect primary region failures. Automatically route to secondary region at request level. No DNS wait. Subsecond traffic shift. But you still need to scale secondary compute to handle full load.

Validation during EKS failover: Check pod health in secondary cluster. Verify services are responding. Test application endpoints. Monitor error rates for spikes indicating issues. If error rates above five percent, investigate before completing failover.

Rollback procedure when failover goes wrong is critical.

Scenario: You promote Aurora to secondary region. Discover critical application bug only present in secondary. Error rates spike to thirty percent. Users flooding support. Continuing on secondary is worse than original outage.

Rollback steps: Demote new primary back to replica role if possible. Or promote original primary again if it's healthy. Update Route53 to point back to original region. Re-enable writes to original region. This can take another two to five minutes. Meanwhile users experience errors.

Prevention: Always test with weighted routing first before full cutover. One percent, five percent, ten percent traffic to secondary. Catch application bugs before going all-in.

Rollback decision tree: If secondary has higher error rate but primary is completely offline, stay on secondary and fix bugs. If primary is recoverable and secondary errors are severe, roll back. If both regions have issues, choose the one with fewer errors and fix both.

Real rollback example: Financial services company failed over to secondary during primary database performance issues. Discovered secondary region's network configuration was wrong, causing intermittent timeouts. Error rate was fifteen percent. Primary was slow but functional at two percent errors. Rolled back to primary, accepted slow performance while fixing network config in secondary. Later failed over successfully after fix.

Chaos engineering to validate DR actually works.

Chaos Monkey principle: Inject random failures continuously. Don't wait for real failures to discover your DR doesn't work. Proactively break things in controlled manner to validate resilience.

For multi-region: Randomly terminate primary Aurora replica. Force failover to secondary. Validate promotion works. Do this monthly. Eventually it becomes routine. Engineers aren't surprised when real failures happen.

Chaos Kong for regional failures: Simulate entire region going offline. Block all traffic from primary region. Force full failover to secondary. Run through complete procedure. Time it. Identify bottlenecks. Update runbooks. Do this quarterly.

Netflix runs Chaos Kong exercises where they take down entire AWS regions intentionally. Engineers practice full regional failover. They discovered issues with cross-region service dependencies, DNS propagation delays, insufficient capacity in secondary regions. Fixed these issues before real regional failures occurred.

Litmus test for DR readiness: Can you execute full regional failover in under ten minutes without incident commander having a panic attack? If yes, your DR works. If no, more practice needed.

Common DR mistakes that extend outages.

Mistake one: No documented runbooks. Team has DR architecture but no written procedures. During outage, engineers Googling "how to promote Aurora read replica". Making it up as they go. Mistakes multiply. Fix: Write detailed runbooks with screenshots, exact commands, validation steps. Store in accessible location. Practice them quarterly.

Mistake two: Runbooks outdated. Wrote runbooks two years ago. Infrastructure changed. Runbooks reference old DNS names, deleted resources, incorrect commands. Following them breaks things further. Fix: Update runbooks after every infrastructure change. Test them quarterly to validate accuracy.

Mistake three: Insufficient practice. Built DR, tested once during initial setup, never again. Real failure happens year later, nobody remembers how to execute. New engineers never practiced. Fix: Quarterly GameDay exercises. All engineers participate. No exceptions.

Mistake four: No rollback plan. Team has failover procedure, no rollback procedure. Failover goes wrong, they panic because they don't know how to undo it. Fix: Document rollback steps alongside failover steps. Test rollback procedures.

Mistake five: Insufficient secondary capacity. Secondary region runs at ten percent capacity to save costs. During failover, auto-scaling should bring it to one hundred percent. But hitting EC2 instance limits, takes ten minutes instead of two. Fix: From Episode 9, run secondary at twenty-five to thirty percent capacity. Request limit increases proactively.

Mistake six: No monitoring during failover. Execute procedures blind without checking if they worked. Promote Aurora but don't verify it's writable. Update DNS but don't confirm propagation. Assume success, actually failed. Fix: Validation step after every action. Use observability from Episode 7 to verify each step.

Before we wrap up, pause and answer these questions.

Question one: Why should you test failover with one percent weighted routing before attempting full cutover?

Question two: During Aurora promotion, how do you prevent data loss from in-flight writes?

Question three: If failover to secondary causes worse errors than original primary issue, what do you do?

Take a moment.

Answers.

Question one: To discover configuration issues and application bugs without affecting all users. One percent traffic to secondary validates it works. If errors spike in that one percent, fix issues before exposing all users. This prevents turning a partial outage into total outage.

Question two: Stop writes to primary before promotion. Put application in read-only mode. Drain in-flight transactions, typically ten to twenty seconds. Then promote. This ensures no writes are in progress that would be lost during the sixty to ninety second promotion window.

Question three: Roll back. If secondary errors are worse than primary issues, demote secondary and promote original primary again. Update DNS back. Accept the original issue was better than the new issues. Meanwhile fix secondary issues, then attempt failover again later.

Let's recap what we covered.

First: Test DR before you need it. Weighted routing with one percent traffic. GameDay exercises quarterly. Chaos engineering with fault injection. Your first failover shouldn't be during a real incident.

Second: Aurora failover procedure has seven steps with validation gates. Check lag, stop writes, promote, verify, update DNS, enable writes, validate end-to-end. Takes two to five minutes for planned failover.

Third: Unplanned failover when primary region is completely offline means promoting immediately without checking lag. Accept potential data loss within RPO. Focus on speed over perfection.

Fourth: Rollback procedures are mandatory. Document them alongside failover procedures. Test them. Have decision criteria for when to roll back versus push forward.

Fifth: Chaos engineering validates DR works. Monthly Chaos Monkey exercises. Quarterly Chaos Kong regional failures. Practice until it's muscle memory.

Sixth: Common mistakes - no runbooks, outdated runbooks, insufficient practice, no rollback plan, insufficient secondary capacity, no monitoring during failover. All preventable with discipline.

Remember Episode 2's hot-warm five-minute RTO? That's one minute Aurora promotion, ninety seconds health check detection, sixty seconds DNS propagation, remaining time for validation. Episode 7's observability shows you what's happening during failover. Episode 8's Route53 health checks trigger automatic failover. All the pieces come together during actual DR execution.

We'll revisit DR costs in Episode 9 context - how much does a DR test cost? How to minimize it? And in Episode 15 anti-patterns, we'll see what happens when teams skip DR testing - the disasters that could have been prevented.

Next time: Financial Services Compliance - SEC SCI, MiFID II, crypto regulations.

Everything we've covered assumes you can architect for your business needs. But regulated industries don't have that luxury. Compliance mandates specific RTO, RPO, data residency, audit logging.

You'll learn SEC Regulation SCI requirements for stock exchanges and how they drive multi-region architecture. MiFID II data residency for EU financial firms. Crypto regulations like NY BitLicense and EU MiCA. How compliance becomes the primary architecture driver, overriding cost considerations. And audit logging that satisfies regulators.

Because in financial services, multi-region isn't optional. It's mandatory. And the requirements are specific.

See you in Episode 13.
