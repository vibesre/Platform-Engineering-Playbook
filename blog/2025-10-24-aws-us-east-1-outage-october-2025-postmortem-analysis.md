---
title: "The $75M/Hour Lesson: AWS US-EAST-1 Outage Postmortem (October 2025)"
description: "Deep technical analysis of the October 2025 AWS outage: DNS race condition in DynamoDB, cascading failures, and why US-EAST-1 is your hidden single point of failure"
keywords:
  - AWS outage October 2025
  - AWS US-EAST-1 postmortem
  - DNS race condition
  - DynamoDB outage
  - cascading failure
  - AWS availability
  - cloud resilience
  - multi-region architecture
  - AWS control plane
  - cloud disaster recovery
  - AWS outage cost
  - distributed systems failure
datePublished: "2025-10-24"
dateModified: "2025-10-24"
slug: 2025-10-24-aws-us-east-1-outage-october-2025-postmortem-analysis
schema:
  type: TechArticle
  questions:
    - question: "What caused the October 2025 AWS outage?"
      answer: "A DNS race condition in DynamoDB's automated DNS management system. Two redundant DNS Enactors experienced a timing collision where one overwrote a new plan with stale data, then cleanup deleted the active plan, leaving an empty DNS record."
    - question: "How long did the AWS US-EAST-1 outage last?"
      answer: "14 hours total from start to finish (October 19, 11:48 PM PDT to October 20, 2:20 PM PDT). DynamoDB recovered in 5.5 hours, but cascading failures in EC2, Lambda, ECS, and other services took another 8+ hours to resolve."
    - question: "How much did the AWS outage cost?"
      answer: "Estimated $75 million per hour across affected companies. For a single application at one company, downtime cost approximately $728,000 for just the DynamoDB outage period (130 minutes at $5,600/minute)."
    - question: "Why does US-EAST-1 affect all AWS regions?"
      answer: "US-EAST-1 serves as the global control plane for most AWS customers (except GovCloud and EU Sovereign Cloud). It handles 35-40% of global AWS traffic and manages control plane operations for workloads in other regions."
    - question: "What is DropletWorkflow Manager congestive collapse?"
      answer: "DWFM manages EC2 instance leases. After DynamoDB recovery, DWFM attempted to re-establish billions of leases simultaneously, overwhelming the system. New instances failed health checks because network configuration was backlogged."
---

# The $75M/Hour Lesson: Inside the 2025 AWS US-EAST-1 Outage

October 19, 2025, 11:48 PM. Ring doorbells stopped recording. Robinhood froze trading. Roblox kicked off millions of players. Medicare enrollment went dark. All at once.

Six and a half million outage reports flooded Downdetector. Over 1,000 companies affected. Seventy [AWS](/technical/aws) services down. Fourteen hours of chaos that exposed a truth most platform engineers didn't want to face: we've built the modern internet on a single region's control plane.

This wasn't a hardware failure. It wasn't human error. It wasn't a cyber attack. It was a latent DNS race condition in DynamoDB's automation‚Äîthe kind of bug that only shows up at scale‚Äîcascading into what became a $75 million per hour lesson in distributed systems design.

> üéôÔ∏è **Listen to the podcast episode**: [The $75M/Hour Lesson: Inside the 2025 AWS US-EAST-1 Outage](/podcasts/00007-aws-outage-october-2025) - Jordan and Alex dissect the technical root cause, AWS's response, and what platform engineers must do Monday morning.

<div style={{position: 'relative', paddingBottom: '56.25%', height: 0, margin: '1.5rem 0'}}>
  <iframe
    style={{position: 'absolute', top: 0, left: 0, width: '100%', height: '100%'}}
    src="https://www.youtube.com/embed/5Xu8jjZ3qjk"
    title="The $75 Million Per Hour Lesson: Inside the 2025 AWS US-EAST-1 Outage"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
  </iframe>
</div>

## Quick Answer (TL;DR)

**Problem**: A DNS race condition in DynamoDB triggered a 14-hour outage affecting 70+ AWS services globally. Most companies discovered hidden dependencies on US-EAST-1's control plane.

**Root Cause**: Dual DNS Enactors designed for redundancy created a timing vulnerability. Unusual delays caused one Enactor to overwrite the active DNS plan with stale data, then cleanup deleted the active record, leaving dynamodb.us-east-1.amazonaws.com with an empty endpoint.

**Cascading Impact**: DynamoDB ‚Üí DropletWorkflow Manager ‚Üí EC2 ‚Üí Lambda/ECS/EKS/Fargate. DWFM entered "congestive collapse" trying to re-establish billions of leases simultaneously.

**Cost**: $75M/hour aggregate across companies. $728K for a single application's 130-minute outage period.

**AWS Response**: Manual DNS restoration (2:25 AM), throttling and selective restarts (4:14 AM), disabled NLB auto-failover (9:36 AM), full resolution (2:20 PM). Globally disabled DynamoDB DNS automation pending safeguards.

**Key Takeaway**: US-EAST-1 handles 35-40% of global AWS traffic and serves as control plane for most customers. Multi-region workloads still depend on US-EAST-1 control plane operations.

## Key Statistics (October 2025 AWS Outage)

| Metric | Value | Source |
|--------|-------|--------|
| Outage reports filed | 6.5 million globally | [Downdetector via NBC News](https://www.nbcnews.com/news/us-news/amazon-web-services-outage-websites-offline-rcna238594) |
| Companies affected | 1,000+ | Multiple news sources |
| AWS services impacted | 70+ | AWS Status Page |
| Start time | October 19, 11:48 PM PDT | [AWS Official Postmortem](https://aws.amazon.com/message/101925/) |
| DynamoDB recovery | October 20, 2:25 AM PDT (5.5 hours) | AWS Official Postmortem |
| Full resolution | October 20, 2:20 PM PDT (14 hours) | AWS Official Postmortem |
| Estimated cost per hour | $75 million | [Tenscope Analysis](https://americanbazaaronline.com/2025/10/21/aws-outage-costed-us-companies-roughly-75-million-per-hour-469016/) |
| Downtime cost per minute (Gartner) | $5,600 per application | Industry benchmark (2014) |
| US-EAST-1 global traffic share | 35-40% | Industry analysis |
| US-EAST-1 launch date | August 2006 (first AWS region) | [Wikipedia](https://en.wikipedia.org/wiki/Timeline_of_Amazon_Web_Services) |

## Timeline: 14 Hours of Cascading Failure

### Phase 1: The DNS Race Condition (11:48 PM - 2:25 AM)

**11:48 PM PDT, October 19**: Customers start seeing increased DynamoDB API error rates in US-EAST-1. At first, it looks like a normal spike.

**Within minutes**: DynamoDB can't resolve its DNS endpoint (dynamodb.us-east-1.amazonaws.com). New connections failing.

**The Technical Failure**: DynamoDB's automated DNS management system has two components working together:

1. **DNS Planner**: Monitors load balancer health, creates DNS plans
2. **DNS Enactor**: Applies plans to Route 53 (runs redundantly in 3 availability zones)

Here's what went wrong:

1. Enactor One starts applying the "Old Plan" but experiences unusual delays (network issues or compute latency)
2. While Enactor One is stuck, Enactor Two sees a "New Plan," applies it successfully
3. Enactor Two triggers cleanup of stale plans‚Äîmarks Old Plan for deletion
4. **Race condition triggers**: Enactor One finishes its delayed operation and OVERWRITES the New Plan with Old Plan data
5. Cleanup process runs, sees "Old Plan is stale, delete it"‚Äîexcept it's actually deleting the active plan
6. **Result**: Empty DNS record for dynamodb.us-east-1.amazonaws.com

**Why recovery failed**: The system entered "an inconsistent state that prevented subsequent plan updates." The safety check designed to prevent older plans from being applied was defeated by the unusual timing.

> **üí° Key Takeaway**
>
> Redundancy designed to prevent failure became the source of failure. This is the paradox of distributed systems at scale‚Äîthe more safeguards you add, the more potential failure modes you create. The race condition had been latent in the system, requiring very specific timing to trigger.

**2:25 AM PDT, October 20**: AWS engineers manually restore DynamoDB's DNS records. Problem solved?

No. The real nightmare was just beginning.

### Phase 2: Congestive Collapse (2:25 AM - 2:20 PM)

**The Secondary Failure**: DynamoDB comes back, but recovery triggers a massive cascade.

**DropletWorkflow Manager (DWFM)**: This subsystem manages physical servers for EC2. It tracks "leases" for each server (droplet) to determine availability.

**When DynamoDB failed**: DWFM couldn't maintain leases for physical servers hosting EC2 instances.

**When DynamoDB recovered**: DWFM tried to re-establish **billions of droplet leases all at once**.

AWS called this "congestive collapse"‚Äîthe system wasn't designed for every lease to fail and recover simultaneously.

**The Cascading Dominos**:

1. **EC2 instances**: Launching but failing health checks (network config backlogged)
2. **Network Load Balancers**: Removing capacity (think instances are unhealthy when they're just not configured yet)
3. **Lambda**: Can't execute (depends on EC2 primitives)
4. **ECS/EKS**: Can't scale clusters (container orchestration broken)
5. **Fargate**: Tasks stuck (serverless compute depends on the same infrastructure)

**4:14 AM**: AWS starts throttling incoming work and selectively restarting DWFM hosts

**9:36 AM**: They disable automatic NLB health check failovers to stop the capacity removal spiral

**11:23 AM**: Begin relaxing request throttles

**1:50 PM**: Full API normalization achieved

**2:20 PM**: Overall resolution confirmed

**Globally**: AWS disabled the DynamoDB DNS automation entirely, worldwide, pending implementation of safeguards.

> **üí° Key Takeaway**
>
> AWS had no established operational recovery procedure for this scenario. The postmortem explicitly states engineers "attempted multiple mitigation steps" without a playbook. They were improvising in real-time for 14 hours. The automation designed for reliability became too dangerous to trust.

## The Real Cost: $75 Million Per Hour

Let's do the math that makes CFOs pay attention.

**Gartner Benchmark**: $5,600 per minute of downtime per application

**For the DynamoDB outage alone** (130 minutes, not including full 14 hours):
- 130 minutes √ó $5,600 = **$728,000 for ONE application at ONE company**

**Multiply across**:
- 1,000+ companies affected
- Many running multiple critical applications
- 14 hours of full or partial outage

**Real-World Impact**:
- **Trading platforms**: Lost transactions during market hours
- **Medicare enrollment**: Offline during open enrollment period
- **Supply chain systems**: Couldn't process orders
- **Gaming platforms**: Roblox offline for millions of users
- **Smart home devices**: Ring doorbells stopped recording
- **Financial services**: Robinhood trading frozen

ParcelHero estimated billions in lost revenue and service disruption across the industry.

> **üí° Key Takeaway**
>
> $75 million per hour makes multi-region architecture look like cheap insurance. Option A: Spend $400/month for managed multi-region platform. Option B: Accept $75M/hour risk exposure. Option C: Spend $15K/month in engineering time building your own. The math suddenly looks very different.

## Why US-EAST-1 Is Your Hidden Single Point of Failure

Here's what surprised a lot of companies: **US-EAST-1 is the control plane for the world**.

### The Architectural Reality

**US-EAST-1 characteristics**:
- AWS's oldest region (launched August 2006 with EC2)
- Handles 35-40% of global AWS traffic
- Most mature, most interconnected, most deeply integrated
- Where all original services were built

**The Control Plane Problem**:

Even if your services run in EU-WEST-1 or AP-SOUTHEAST-1, they depend on US-EAST-1 for control plane operations.

**Companies thought they were multi-region**:
- ‚úì Workloads deployed to multiple regions
- ‚úì Traffic routing across geographies
- ‚úó Discovered hidden US-EAST-1 dependencies

**Your compute might be in EU-WEST-1. The control plane managing it? That's in US-EAST-1.**

### Why This Architecture Exists

**Two reasons**:

1. **Backwards compatibility**: AWS has built 200+ services over 19 years. Moving the control plane would be a massive undertaking with enormous risk.

2. **Operational complexity**: The interconnections are so deep that partial migration could create even more failure modes.

**Exceptions**:
- AWS GovCloud (US): Separate control plane for government workloads
- EU Sovereign Cloud: Separate control plane for EU data residency requirements

For everyone else? US-EAST-1 is your control plane whether you know it or not.

> **üí° Key Takeaway**
>
> This is an architectural constant you must design around. AWS isn't changing the fundamental architecture‚ÄîUS-EAST-1 remains the global control plane. They're adding better guardrails, but the systemic risk remains. You can't rely on AWS to architect away this risk. You have to own your resilience strategy.

## What AWS Is Fixing (And What They're Not)

### Immediate Actions

**DNS Race Condition Prevention**:
- Adding safeguards to prevent timing-based overwrites
- Improved staleness detection for delayed operations
- Better synchronization between redundant Enactors

**Cascading Failure Mitigation**:
- Rate limiting on NLB health check failovers (prevent capacity removal spirals)
- Additional test suites for DWFM to prevent regressions
- Improved queue management for lease re-establishment

**Operational Improvements**:
- Recovery playbooks for DWFM congestive collapse
- Better monitoring and alerting for DNS management state
- Procedures for gradual recovery vs. all-at-once surge

### What's NOT Changing

**US-EAST-1 as Global Control Plane**: Not moving. The centralized model that created the vulnerability is staying.

**Fundamental Architecture**: Just adding better guardrails to the existing design.

**Your Responsibility**: The systemic risk remains. Platform engineers must own their resilience strategy.

## Three Monday Morning Questions for Platform Engineers

### Question 1: What's Our Single-Region Exposure?

**Don't just ask**: "Where do our workloads run?"

**Ask instead**: "What fails if US-EAST-1 goes dark?"

**Action Items**:
- Map the critical paths through your architecture
- Document dependencies you didn't know existed
- Trace control plane operations, not just data plane
- Test assumptions about "multi-region" deployments

**Reality check**: A lot of teams are about to discover they're not as multi-region as they thought.

### Question 2: What's Our $75M/Hour Calculation?

**The Formula**:
```
Risk Exposure = Downtime Cost √ó Probability √ó Expected Duration
```

**For your business**:
1. Calculate downtime cost per minute (revenue + customer impact + SLA penalties)
2. Estimate probability of US-EAST-1 failure (3 major outages in 5 years)
3. Multiply by expected duration (6-14 hours based on history)

**Compare to**:
- Cost of multi-region architecture
- Cost of managed resilience platforms
- Cost of engineering time to build it yourself

**Example ROI**:
- **Risk exposure**: $100K/hour √ó 0.6 incidents/year √ó 10 hours = $600K annual risk
- **Multi-region cost**: $5K/month platform + $10K/month engineering = $180K annual
- **Net benefit**: $420K/year + eliminated risk

Suddenly that $400/month platform doesn't look expensive.

### Question 3: Do We Have Cascading Failure Playbooks?

**AWS didn't.** That's why manual intervention took so long.

**Ask your team**:
- Can we recover when automation fails?
- Have we tested this in game days?
- Do we have runbooks for manual recovery?
- Can we operate in degraded mode?

**Testing Framework**:
1. **Tabletop exercise**: Walk through US-EAST-1 failure scenarios
2. **Chaos engineering**: Inject failures in staging
3. **DR drills**: Actually fail over to backup region
4. **Recovery metrics**: Measure RTO/RPO, don't just estimate

> **üí° Key Takeaway**
>
> The lesson isn't "avoid AWS." All systems fail eventually. The lesson is that we've built a centralized internet on a single region's control plane, and you must own your resilience strategy. The $75M/hour lesson makes the business case obvious.

## Practical Actions This Week

### For Individual Engineers

**Study the postmortem like a textbook**:
- This is how race conditions work in distributed systems
- This is how cascading failures propagate
- This is how DNS caching delays recovery

**Skill development**:
- Add resilience engineering to your roadmap
- Learn multi-region architecture patterns
- Understand control plane vs. data plane dependencies
- This is becoming a specialization (and one that pays well)

**Career positioning**:
- Engineers who understand distributed systems failure modes are increasingly valuable
- Resilience architecture is a $200K+ skill set
- Every company will be asking these questions after this outage

### For Platform Teams

**This Week**:
1. **Schedule a DR drill**: Pretend AWS US-EAST-1 is down. What breaks?
2. **Map dependencies**: Find the single-region dependencies you didn't know about
3. **Test failover**: Can you actually switch regions? How long does it take?
4. **Price alternatives**: Get quotes for multi-region options

**The business case just got a lot clearer**.

**Next Month**:
1. Build actual runbooks for degraded mode operations
2. Implement chaos engineering to test assumptions
3. Start migrating critical workloads to multi-region

### For Leadership

**Use this incident to unblock resilience budget**.

**The Argument**:
- $75 million per hour happened to companies just like us
- Investment in resilience is insurance with clear ROI
- This is the wake-up call to prioritize what we've been postponing

**The Ask**:
- Budget for multi-region architecture
- Engineering time for resilience improvements
- Tools and platforms for automated failover
- Training and skill development for the team

**The Timeline**:
- Phase 1 (Q1): Assessment and planning
- Phase 2 (Q2): Critical workload migration
- Phase 3 (Q3): Full multi-region capabilities
- Phase 4 (Q4): Chaos testing and validation

## The Bigger Picture: Centralized Cloud's Achilles Heel

October 20, 2025, will be remembered as the day the cloud showed its Achilles heel.

Not that AWS failed‚Äîall systems fail eventually. But that **we've built a centralized internet on a single region's control plane**.

### The Paradox We Face

**Centralization enables**:
- Incredible scale and efficiency
- Consistent global control plane
- Simplified operations for AWS
- Lower costs through shared infrastructure

**Centralization creates**:
- Single points of failure
- Cascading failure potential
- Global impact from regional issues
- Systemic risk you can't opt out of

### The Path Forward

**This isn't about abandoning AWS**. It's about understanding the architecture you're building on and owning the risk.

**The next outage isn't a question of if. It's when.**

**Will you be ready?**

The fundamentals of good engineering remain constant:
- **Understand your dependencies** (especially the hidden ones)
- **Plan for failure** (not if, when)
- **Don't let convenience override resilience**

The $75 million per hour lesson makes the business case obvious.

## üìö Learning Resources

### Official Documentation & Postmortems
- [AWS Official Postmortem](https://aws.amazon.com/message/101925/) - Complete technical root cause analysis
- [AWS Well-Architected Framework - Reliability Pillar](https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/welcome.html)
- [AWS Multi-Region Application Architecture](https://aws.amazon.com/solutions/implementations/multi-region-application-architecture/)

### Technical Analysis
- [The Register: Single DNS Race Condition Brought AWS to its Knees](https://www.theregister.com/2025/10/23/amazon_outage_postmortem/)
- [Pragmatic Engineer: What Caused the Large AWS Outage?](https://newsletter.pragmaticengineer.com/p/what-caused-the-large-aws-outage)
- [ByteSizedDesign: The AWS October 20th Outage Dissection](https://bytesizeddesign.substack.com/p/the-aws-october-20th-outage-dissection)

### Resilience Engineering
- "Site Reliability Engineering" by Google - [Free online](https://sre.google/sre-book/table-of-contents/) | [Purchase on Amazon](https://www.amazon.com/Site-Reliability-Engineering-Production-Systems/dp/149192912X)
- "Chaos Engineering" by Netflix - [Official PDF](https://principlesofchaos.org/)
- [AWS Disaster Recovery Guide](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-workloads-on-aws.html)

### Multi-Region Architecture Patterns
- [AWS Multi-Region Terraform Deployment](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/guides/using-multiple-regions)
- [AWS Global Accelerator for Multi-Region Failover](https://aws.amazon.com/global-accelerator/)
- [Netflix Multi-Region Architecture](https://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b)

### Community Discussion
- [Hacker News: AWS Outage Discussion](https://news.ycombinator.com/) - Search "AWS October 2025 outage"
- [r/devops AWS Outage Megathread](https://www.reddit.com/r/devops/)
- [AWS re:Post Community Forums](https://repost.aws/)

---

**Related Content**:
- [AWS: Strategic Cloud Platform Guide](/technical/aws)
- [Platform Engineering Economics: Hidden Costs & ROI](/blog/2025-01-platform-engineering-economics-hidden-costs-roi)
- [Podcast Episode: AWS State of the Union 2025](/podcasts/00006-aws-state-of-the-union-2025)

---

*Last updated: October 24, 2025*
